{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8475824,"sourceType":"datasetVersion","datasetId":5053999},{"sourceId":8496192,"sourceType":"datasetVersion","datasetId":5069679},{"sourceId":8496944,"sourceType":"datasetVersion","datasetId":5070227},{"sourceId":8596421,"sourceType":"datasetVersion","datasetId":5081117},{"sourceId":8648676,"sourceType":"datasetVersion","datasetId":5148594},{"sourceId":8649077,"sourceType":"datasetVersion","datasetId":5116967},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import json\nimport pandas as pd\n\ndef read_json(file_path: str) -> dict:\n    \"\"\"Read a json file and return a dict.\"\"\"\n    with open(file_path) as f:\n        data = json.load(f)\n    return data\n\nresponse = read_json('/kaggle/input/retriever-responses/cosine_sim_response.json')\ncorpus = read_json('/kaggle/input/wiki-data/wiki_musique_corpus.json')\ndev = read_json('/kaggle/input/wiki-data/dev.json')[:1200]\n\n# faster way to index the data\ndev_df = pd.DataFrame(dev)\ndev_df.set_index('_id', inplace=True)\n\ndef get_ground_truth(idx, supporting_facts=False):\n    \"\"\"Extract the question, oracle contexts and answer from dev.\n    The oracle contexts are tuples (title, text).\n    \"\"\"\n    row = dev_df.loc[idx]\n    if supporting_facts:\n        sfs = [t for t, _ in row['supporting_facts']]\n        oracle = [(title, ' '.join(texts)) for title, texts in row['context'] if title in sfs]\n    else:\n        oracle = [(title, ' '.join(texts)) for title, texts in row['context']]\n    return row['question'], oracle, row['answer']\n\ndef get_top_k_contexts(responses, k=5):\n    \"\"\"Extract the top k similar contexts from the corpus. Each\n    context is a tuple (title, text).\n    \"\"\"\n    top_k_contexts = []\n    for i, (idx, score) in enumerate(responses.items()):\n        if i < k:\n            tittle = corpus[idx]['title']\n            text = corpus[idx]['text']\n            top_k_contexts.append((tittle, text))\n    return top_k_contexts\n\ndef create_prompt(contexts, question):\n    \"\"\"Create a prompt for the model.\"\"\"\n    prompt = \"Documents:\\n\"\n    for i, (title, text) in enumerate(contexts):\n        prompt += f\"Document[{i+1}](Title: {title}) {text}\\n\"\n    prompt += f\"\\nQuestion: {question}\"\n    return prompt\n\nresults = []\nquestions = []\nfor _id, similar_ctx_ids in response.items():\n    question, oracle_ctxs, answer = get_ground_truth(_id, supporting_facts=True)\n    questions.append(question)\n    similar_top1_ctxs = get_top_k_contexts(similar_ctx_ids, k=1)\n    similar_top3_ctxs = get_top_k_contexts(similar_ctx_ids, k=3)\n    similar_top5_ctxs = get_top_k_contexts(similar_ctx_ids, k=5)\n    \n    # create prompts\n    prompt_oracle = create_prompt(oracle_ctxs, question)\n    prompt_top1_similar = create_prompt(similar_top1_ctxs, question)\n    prompt_top3_similar = create_prompt(similar_top3_ctxs, question)\n    prompt_top5_similar = create_prompt(similar_top5_ctxs, question)\n    \n    # save prompts in a list\n    results.append({\n        \"prompt_oracle\": prompt_oracle,\n        \"prompt_top1_similar\": prompt_top1_similar,\n        \"prompt_top3_similar\": prompt_top3_similar,\n        \"prompt_top5_similar\": prompt_top5_similar,\n        \"answer\": answer\n    })\n\n# save results in json file with nice indentation\nwith open('/kaggle/working/prompts.json', 'w') as f:\n    json.dump(results, f, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\nimport torch\nimport transformers\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TextDataset, DataCollatorForLanguageModeling\nfrom IPython.display import display, Markdown\nfrom torch.utils.data import DataLoader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def query_model_batch(system_message, user_messages, temperature=0.7, max_length=1024):\n    start_time = time()\n    batch_prompts = []\n    for user_message in user_messages:\n        user_message = user_message + \" Answer:\"\n        messages = [\n            {\"role\": \"system\", \"content\": system_message},\n            {\"role\": \"user\", \"content\": user_message},\n        ]\n        prompt = pipeline.tokenizer.apply_chat_template(\n            messages, \n            tokenize=False, \n            add_generation_prompt=True\n        )\n        batch_prompts.append(prompt)\n\n    terminators = [\n        pipeline.tokenizer.eos_token_id,\n        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n\n    # Generate sequences in batch\n    sequences = pipeline(\n        batch_prompts,\n        do_sample=True,\n        top_p=0.9,\n        temperature=temperature,\n        eos_token_id=terminators,\n        max_new_tokens=max_length,\n        return_full_text=False,\n        pad_token_id=pipeline.model.config.eos_token_id\n    )\n    \n    responses = [seq[0]['generated_text'] for seq in sequences]\n    end_time = time()\n    ttime = f\"Total time: {round(end_time-start_time, 2)} sec.\"\n\n    return responses, ttime\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def colorize_text(text):\n    for word, color in zip([\"Documents\", \"Question\", \"Answer\", \"Total time\"], [\"blue\",\"red\", \"green\", \"magenta\"]):\n        text = text.replace(f\"{word}:\", f\"\\n\\n**<font color='{color}'>{word}:</font>**\")\n    return text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompts_oracle = [results[i]['prompt_oracle'] for i in range(len(results))]\nprompts_top_1 = [results[i]['prompt_top1_similar'] for i in range(len(results))]\nprompts_top_3 = [results[i]['prompt_top3_similar'] for i in range(len(results))]\nprompts_top_5 = [results[i]['prompt_top5_similar'] for i in range(len(results))]\nanswers = [results[i]['answer'] for i in range(len(results))]\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset:\n    def __init__(self, prompts, answers):\n        self.prompts = prompts\n        self.answers = answers\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __getitem__(self, idx):\n        return {\"prompt\": self.prompts[idx], \"answer\": self.answers[idx]}\n\ndataset = CustomDataset(prompts_oracle, answers)\noracle_dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\ndataset = CustomDataset(prompts_top_1, answers)\ntop1_dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\ndataset = CustomDataset(prompts_top_3, answers)\ntop3_dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\ndataset = CustomDataset(prompts_top_5, answers)\ntop5_dataloader = DataLoader(dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install langchain","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain.evaluation import ExactMatchStringEvaluator\n\nevaluator = ExactMatchStringEvaluator(\n    ignore_case=True,\n    ignore_punctuation=True,\n)\n\npremessage = \"\"\"\nYou have to answer complex questions based on the provided contexts.\nRespond with as few tokens as possible. You don't need to explain your answer. \nDon't add any extra information.\n\"\"\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exact_match_result = 0\ndictionary = {}\n\nfor batch in oracle_dataloader:\n    batch_prompts = batch['prompt']\n    batch_answers = batch['answer']\n    responses, ttime = query_model_batch(premessage, batch_prompts, temperature=0.1, max_length=32)\n    \n    for i in range(len(responses)):\n        response = responses[i]\n        answer = batch_answers[i]\n        \n        #print(\"Generated: \", response)\n        #print(\"Correct: \", answer)\n        \n        dictionary[batch_prompts[i]] = [response, answer]\n        ex = float(evaluator.evaluate_strings(prediction=response, reference=answer)['score'])\n        exact_match_result += ex\n        \n\nprint(\"Oracle: \", exact_match_result/1200)\n\nwith open('/kaggle/working/oracle_esit.json', 'w') as f:\n    json.dump(dictionary, f, indent=4)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exact_match_result = 0\ndictionary = {}\n\nfor batch in top1_dataloader:\n    batch_prompts = batch['prompt']\n    batch_answers = batch['answer']\n    responses, ttime = query_model_batch(premessage, batch_prompts, temperature=0.1, max_length=32)\n    \n    for i in range(len(responses)):\n        response = responses[i]\n        answer = batch_answers[i]\n        \n        #print(\"Generated: \", response)\n        #print(\"Correct: \", answer)\n        \n        dictionary[batch_prompts[i]] = [response, answer]\n        ex = float(evaluator.evaluate_strings(prediction=response, reference=answer)['score'])\n        exact_match_result += ex\n        \n\nprint(\"Top-1: \", exact_match_result/1200)\n\nwith open('/kaggle/working/top1_esit.json', 'w') as f:\n    json.dump(dictionary, f, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exact_match_result = 0\ndictionary = {}\n\nfor batch in top3_dataloader:\n    batch_prompts = batch['prompt']\n    batch_answers = batch['answer']\n    responses, ttime = query_model_batch(premessage, batch_prompts, temperature=0.1, max_length=32)\n    \n    for i in range(len(responses)):\n        response = responses[i]\n        answer = batch_answers[i]\n        \n        #print(\"Generated: \", response)\n        #print(\"Correct: \", answer)\n        \n        dictionary[batch_prompts[i]] = [response, answer]\n        ex = float(evaluator.evaluate_strings(prediction=response, reference=answer)['score'])\n        exact_match_result += ex\n        \n\nprint(\"Top-3: \", exact_match_result/1200)\n\nwith open('/kaggle/working/top3_esit.json', 'w') as f:\n    json.dump(dictionary, f, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exact_match_result = 0\ndictionary = {}\n\nfor batch in top5_dataloader:\n    batch_prompts = batch['prompt']\n    batch_answers = batch['answer']\n    responses, ttime = query_model_batch(premessage, batch_prompts, temperature=0.1, max_length=32)\n    \n    for i in range(len(responses)):\n        \n        response = responses[i]\n        answer = batch_answers[i]\n        \n        print(\"Generated: \", response)\n        print(\"Correct: \", answer)\n        \n        dictionary[batch_prompts[i]] = [response, answer]\n        ex = float(evaluator.evaluate_strings(prediction=response, reference=answer)['score'])\n        exact_match_result += ex\n        \n\nprint(\"Top-5: \", exact_match_result/1200)\n\nwith open('/kaggle/working/top5_esit.json', 'w') as f:\n    json.dump(dictionary, f, indent=4)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}