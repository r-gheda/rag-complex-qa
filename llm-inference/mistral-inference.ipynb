{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8496944,"sourceType":"datasetVersion","datasetId":5070227},{"sourceId":8659375,"sourceType":"datasetVersion","datasetId":5187974},{"sourceId":5112,"sourceType":"modelInstanceVersion","modelInstanceId":3900}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U transformers\n!pip install -q -U accelerate\n!pip install -q -U bitsandbytes\n!pip install -q -U accelerate\n!pip install -i https://pypi.org/simple/ bitsandbytes","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-10T17:43:59.280062Z","iopub.execute_input":"2024-06-10T17:43:59.280354Z","iopub.status.idle":"2024-06-10T17:45:24.888921Z","shell.execute_reply.started":"2024-06-10T17:43:59.280307Z","shell.execute_reply":"2024-06-10T17:45:24.887958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, pipeline\nimport torch\nimport json","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:45:24.891056Z","iopub.execute_input":"2024-06-10T17:45:24.891341Z","iopub.status.idle":"2024-06-10T17:45:44.855866Z","shell.execute_reply.started":"2024-06-10T17:45:24.891301Z","shell.execute_reply":"2024-06-10T17:45:44.854855Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = \"/kaggle/input/mistral/pytorch/7b-instruct-v0.1-hf/1\"\npipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:45:44.857056Z","iopub.execute_input":"2024-06-10T17:45:44.857777Z","iopub.status.idle":"2024-06-10T17:47:22.587616Z","shell.execute_reply.started":"2024-06-10T17:45:44.857746Z","shell.execute_reply":"2024-06-10T17:47:22.586710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from time import time\n\ndef query_model_batch(task_instruction, user_messages, temperature=0.7, max_length=1024):\n    batch_prompts = []\n    for um in user_messages:\n        user_mssg = task_instruction + um + \" Answer:\"\n        messages = [\n            {\"role\": \"user\", \"content\": user_mssg}\n        ]\n        prompt = pipeline.tokenizer.apply_chat_template(\n            messages, \n            tokenize=False, \n            add_generation_prompt=True\n        )\n        batch_prompts.append(prompt)\n\n    terminators = [\n        pipeline.tokenizer.eos_token_id,\n        pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n    ]\n\n    # Generate sequences in batch\n    sequences = pipeline(\n        batch_prompts,\n        do_sample=True,\n        top_p=0.9,\n        temperature=temperature,\n        eos_token_id=terminators,\n        max_new_tokens=max_length,\n        return_full_text=False,\n        pad_token_id=pipeline.model.config.eos_token_id\n    )\n    \n    responses = [seq[0]['generated_text'] for seq in sequences]\n    return responses","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:53:32.996970Z","iopub.execute_input":"2024-06-10T17:53:32.997802Z","iopub.status.idle":"2024-06-10T17:53:33.005447Z","shell.execute_reply.started":"2024-06-10T17:53:32.997770Z","shell.execute_reply":"2024-06-10T17:53:33.004470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\n\n# load the data\ndef read_json(file_path: str) -> dict:\n    \"\"\"Read a json file and return a dict.\"\"\"\n    with open(file_path) as f:\n        data = json.load(f)\n    return data\n\n# custom torch dataset\nclass RAGDataset(Dataset):\n    def __init__(self, prompts, answers):\n        self.prompts = prompts\n        self.answers = answers\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __getitem__(self, idx):\n        return self.prompts[idx], self.answers[idx]\n    \ntask_instruction = \"\"\"\nYou have to answer complex questions based on the provided contexts.\nRespond with as few tokens as possible. You don't need to explain your answer. \nDon't add any extra information.\n\"\"\"\n\ndef get_llm_response(data_path_llm_prompts, save_to_file=None):\n    llm_prompts_dict = read_json(data_path_llm_prompts)\n    \n    # extract info.\n    prompts = [v[\"prompt\"] for k, v in llm_prompts_dict.items()]\n    answers = [v[\"answer\"] for k, v in llm_prompts_dict.items()]\n    \n    # create dataset and dataloader\n    dataset = RAGDataset(prompts, answers)\n    loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=4)\n    \n    # get response from LLM in batches\n    llm_responses = {}\n    for batch_prompts, batch_answers in tqdm(loader):\n        batch_responses = query_model_batch(task_instruction, batch_prompts, temperature=0.1, max_length=300)\n\n        for i in range(len(batch_responses)):\n            prompt = batch_prompts[i]\n            response = batch_responses[i]\n            answer = batch_answers[i]\n\n            llm_responses[prompt] = [response, answer]\n    \n    # save response to json\n    if save_to_file:\n        with open(f\"/kaggle/working/{save_to_file}.json\", 'w') as f:\n            json.dump(llm_responses, f, indent=4)","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:53:36.000473Z","iopub.execute_input":"2024-06-10T17:53:36.000835Z","iopub.status.idle":"2024-06-10T17:53:36.013747Z","shell.execute_reply.started":"2024-06-10T17:53:36.000809Z","shell.execute_reply":"2024-06-10T17:53:36.012715Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_llm_response(data_path_llm_prompts=\"path-to-prompts\", save_to_file=\"save-name\")","metadata":{"execution":{"iopub.status.busy":"2024-06-10T17:54:49.903617Z","iopub.execute_input":"2024-06-10T17:54:49.904462Z","iopub.status.idle":"2024-06-10T18:13:51.635177Z","shell.execute_reply.started":"2024-06-10T17:54:49.904431Z","shell.execute_reply":"2024-06-10T18:13:51.634175Z"},"trusted":true},"execution_count":null,"outputs":[]}]}