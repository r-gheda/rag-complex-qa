{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8475824,"sourceType":"datasetVersion","datasetId":5053999},{"sourceId":8596421,"sourceType":"datasetVersion","datasetId":5081117},{"sourceId":8657743,"sourceType":"datasetVersion","datasetId":5148803},{"sourceId":4265,"sourceType":"modelInstanceVersion","modelInstanceId":3048}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom string import Template\nfrom torch.utils.data import DataLoader","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-04T20:14:52.619347Z","iopub.execute_input":"2024-06-04T20:14:52.619692Z","iopub.status.idle":"2024-06-04T20:14:59.976813Z","shell.execute_reply.started":"2024-06-04T20:14:52.619663Z","shell.execute_reply":"2024-06-04T20:14:59.976003Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llm = '/kaggle/input/flan-t5/pytorch/xl/3'\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel = T5ForConditionalGeneration.from_pretrained(llm, device_map=\"auto\")\ntokenizer = T5Tokenizer.from_pretrained(llm)\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T20:14:59.978564Z","iopub.execute_input":"2024-06-04T20:14:59.978999Z","iopub.status.idle":"2024-06-04T20:17:08.317306Z","shell.execute_reply.started":"2024-06-04T20:14:59.978947Z","shell.execute_reply":"2024-06-04T20:17:08.316334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate(prompt):\n    inputs = tokenizer(prompt, return_tensors=\"pt\")\n    outputs = model.generate(**inputs)\n    return tokenizer.batch_decode(outputs, skip_special_tokens=True)\n\n\ndef query_model_batch(premessage, batch_prompts):\n    responses = []\n    for prompt in batch_prompts:\n        responses.append(generate(premessage + prompt))\n    return responses","metadata":{"execution":{"iopub.status.busy":"2024-06-04T20:18:29.262250Z","iopub.execute_input":"2024-06-04T20:18:29.263576Z","iopub.status.idle":"2024-06-04T20:18:29.269731Z","shell.execute_reply.started":"2024-06-04T20:18:29.263531Z","shell.execute_reply":"2024-06-04T20:18:29.268607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CustomDataset:\n    def __init__(self, prompts, answers):\n        self.prompts = prompts\n        self.answers = answers\n\n    def __len__(self):\n        return len(self.prompts)\n\n    def __getitem__(self, idx):\n        return {\"prompt\": self.prompts[idx], \"answer\": self.answers[idx]}\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-04T20:18:29.628489Z","iopub.execute_input":"2024-06-04T20:18:29.628896Z","iopub.status.idle":"2024-06-04T20:18:29.635811Z","shell.execute_reply.started":"2024-06-04T20:18:29.628858Z","shell.execute_reply":"2024-06-04T20:18:29.634718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"premessage = \"\"\"\nYou have to answer complex questions based on the provided contexts.\nRespond with as few tokens as possible. You don't need to explain your answer. \nDon't add any extra information.\n\"\"\"\n\nimport json\nimport pandas as pd\n\ndef read_json(file_path: str) -> dict:\n    \"\"\"Read a json file and return a dict.\"\"\"\n    with open(file_path) as f:\n        data = json.load(f)\n    return data\n\nresponse = read_json('/kaggle/input/rag-retrieve-responses/contriever_top1_adore2.json')\n#response = read_json('/kaggle/input/r-and-h/random_contexts.json')\ncorpus = read_json('/kaggle/input/wiki-data/wiki_musique_corpus.json')\ndev = read_json('/kaggle/input/wiki-data/dev.json')[:1200]\n\n# faster way to index the data\ndev_df = pd.DataFrame(dev)\ndev_df.set_index('_id', inplace=True)\n\ndef get_ground_truth(idx, supporting_facts=False):\n    \"\"\"Extract the question, oracle contexts and answer from dev.\n    The oracle contexts are tuples (title, text).\n    \"\"\"\n    row = dev_df.loc[idx]\n    if supporting_facts:\n        sfs = [t for t, _ in row['supporting_facts']]\n        oracle = [(title, ' '.join(texts)) for title, texts in row['context'] if title in sfs]\n    else:\n        oracle = [(title, ' '.join(texts)) for title, texts in row['context']]\n    return row['question'], oracle, row['answer']\n\ndef get_top_k_contexts(responses, k=5):\n    \"\"\"Extract the top k similar contexts from the corpus. Each\n    context is a tuple (title, text).\n    \"\"\"\n    top_k_contexts = []\n    for i, (idx, score) in enumerate(responses.items()):\n        if i < k:\n            tittle = corpus[idx]['title']\n            text = corpus[idx]['text']\n            top_k_contexts.append((tittle, text))\n    return top_k_contexts\n\ndef create_prompt(contexts, question):\n    \"\"\"Create a prompt for the model.\"\"\"\n    prompt = \"Documents:\\n\"\n    for i, (title, text) in enumerate(contexts):\n        prompt += f\"Document[{i+1}](Title: {title}) {text}\\n\"\n    prompt += f\"\\nQuestion: {question}\"\n    return prompt\n\n\nresults = []\nquestions = []\nfor _id, similar_ctx_ids in response.items():\n    question, oracle_ctxs, answer = get_ground_truth(_id, supporting_facts=True)\n    questions.append(question)\n    \n    prompt_top5_similar =  premessage +similar_ctx_ids['prompt']\n    \n    # save prompts in a list\n    results.append({\n        \"prompt_top5_similar\": prompt_top5_similar,\n        \"answer\": answer\n    })\n\n\n# save results in json file with nice indentation\nwith open('/kaggle/working/prompts.json', 'w') as f:\n    json.dump(results, f, indent=4)\n    \nprompts_top_5 = [results[i]['prompt_top5_similar'] for i in range(len(results))]\n\nanswers = [results[i]['answer'] for i in range(len(results))]","metadata":{"execution":{"iopub.status.busy":"2024-06-04T20:17:33.552211Z","iopub.status.idle":"2024-06-04T20:17:33.552620Z","shell.execute_reply.started":"2024-06-04T20:17:33.552427Z","shell.execute_reply":"2024-06-04T20:17:33.552442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = CustomDataset(prompts_top_5, answers)\ntop5_dataloader = DataLoader(dataset, batch_size=16, shuffle=False)\n\ndictionary = {}\ncounter = 0\n\nfor batch in top5_dataloader:\n    batch_prompts = batch['prompt']\n    batch_answers = batch['answer']\n    responses = query_model_batch(premessage, batch_prompts)\n    counter += len(batch_prompts)\n    print(counter)\n    \n    for i in range(len(responses)):\n        \n        response = responses[i][0]\n        answer = batch_answers[i]\n        \n        dictionary[batch_prompts[i]] = [response, answer]\n         \n\n\nwith open('/kaggle/working/contriever-top1-2adore.json', 'w') as f:\n    json.dump(dictionary, f, indent=4)\nprint(\"Done!\")","metadata":{"execution":{"iopub.status.busy":"2024-06-04T20:17:33.553924Z","iopub.status.idle":"2024-06-04T20:17:33.554304Z","shell.execute_reply.started":"2024-06-04T20:17:33.554134Z","shell.execute_reply":"2024-06-04T20:17:33.554148Z"},"trusted":true},"execution_count":null,"outputs":[]}]}