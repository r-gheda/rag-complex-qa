import json
from langchain.evaluation import ExactMatchStringEvaluator
import nltk
import pandas as pd
from nltk.translate.meteor_score import meteor_score as meteor
from rouge_score import rouge_scorer
from bert_score import score as bert_score

    
# Initialize the ExactMatchStringEvaluator
evaluator = ExactMatchStringEvaluator(
    ignore_case=True,
    ignore_punctuation=True,
)

# Initialize ROUGE scorer
rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)

def evaluate_llm_responses(data):
    """ Evaluate the responses generated by the LLM model using 
    Exact Match, METEOR, ROUGE, and BERT score.
    """   

    # Initialize the results
    exact_match_result = 0
    total_meteor_score = 0
    total_rouge1_score = 0
    total_rouge2_score = 0
    total_rougeL_score = 0
    all_responses = []
    all_answers = []

    # Iterate through the data and compute the metrics
    for question, (response, answer) in data.items():
        response = response.strip()
        # Calculate Exact Match
        ex = float(evaluator.evaluate_strings(prediction=response, reference=answer)['score'])
        exact_match_result += ex

        # meteor_scr = meteor([answer], response)
        # total_meteor_score += meteor_scr

        # Calculate ROUGE
        rouge_scores = rouge.score(response, answer)
        total_rouge1_score += rouge_scores['rouge1'].fmeasure
        total_rouge2_score += rouge_scores['rouge2'].fmeasure
        total_rougeL_score += rouge_scores['rougeL'].fmeasure

        # Collect responses and answers for BERTScore
        all_responses.append(response)
        all_answers.append(answer)

    # Calculate BERTScore for all responses and answers
    P, R, F1 = bert_score(all_responses, all_answers, lang="en")

    # Aggregate BERTScore results
    total_bert_precision = sum(P.tolist())
    total_bert_recall = sum(R.tolist())
    total_bert_f1 = sum(F1.tolist())

    # Final results
    num_samples = len(data)
    exact_match_result /= num_samples
    total_meteor_score /= num_samples
    total_rouge1_score /= num_samples
    total_rouge2_score /= num_samples
    total_rougeL_score /= num_samples
    total_bert_precision /= num_samples
    total_bert_recall /= num_samples
    total_bert_f1 /= num_samples

    return {
        "Exact Match": exact_match_result,
        "METEOR": total_meteor_score,
        "ROUGE-1": total_rouge1_score,
        "ROUGE-2": total_rouge2_score,
        "ROUGE-L": total_rougeL_score,
        "BERT Precision": total_bert_precision,
        "BERT Recall": total_bert_recall,
        "BERT F1": total_bert_f1
    }
    
def read_json(file_path):
    with open(file_path, 'r') as f:
        data = json.load(f)
    return data

# load data
oracle_random_1_answers = read_json('llm-responses/mistral/oracle_random_1_answers.json')
oracle_random_2_answers = read_json('llm-responses/mistral/oracle_random_2_answers.json')
oracle_random_3_answers = read_json('llm-responses/mistral/oracle_random_3_answers.json')

# get metrics
oracle_random_1_metrics = evaluate_llm_responses(oracle_random_1_answers)
oracle_random_2_metrics = evaluate_llm_responses(oracle_random_2_answers)
oracle_random_3_metrics = evaluate_llm_responses(oracle_random_3_answers)

# combine metrics into one pandas dataframe
df = pd.DataFrame([oracle_random_1_metrics, oracle_random_2_metrics, oracle_random_3_metrics])
df.index = ['Oracle + 1 Random', 'Oracle + 2 Random', 'Oracle + 3 Random']

# round to 

# print latex table
print(df.to_latex())