{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5b75191a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-31T16:17:21.040590Z",
     "iopub.status.busy": "2024-05-31T16:17:21.040149Z",
     "iopub.status.idle": "2024-05-31T16:17:38.023016Z",
     "shell.execute_reply": "2024-05-31T16:17:38.021795Z",
     "shell.execute_reply.started": "2024-05-31T16:17:21.040553Z"
    },
    "papermill": {
     "duration": 0.006482,
     "end_time": "2024-06-02T11:40:29.531513",
     "exception": false,
     "start_time": "2024-06-02T11:40:29.525031",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac12a7dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T11:40:29.550216Z",
     "iopub.status.busy": "2024-06-02T11:40:29.549848Z",
     "iopub.status.idle": "2024-06-02T11:40:46.297548Z",
     "shell.execute_reply": "2024-06-02T11:40:46.296534Z"
    },
    "papermill": {
     "duration": 16.761935,
     "end_time": "2024-06-02T11:40:46.299614",
     "exception": false,
     "start_time": "2024-06-02T11:40:29.537679",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-02 11:40:35.844771: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-06-02 11:40:35.844879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-06-02 11:40:35.982636: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# Copyright 2018 The Open AI Team Authors and The HuggingFace Inc. team.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "This file is copied from Huggingface Transformers 2.8.0 because we find\n",
    "RobertaTokenizer behaves differently when the library updates to version\n",
    "3 and 4. To replicate JPQ (CIKM'21) as well as STAR/ADORE (SIGIR'21), it\n",
    "is necessary to use the RobertaTokenizer in this file or the one defined \n",
    "in Transformers 2.x version.\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import json\n",
    "import os\n",
    "import regex as re\n",
    "from functools import lru_cache\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "import copy\n",
    "import itertools\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from tokenizers.implementations import BaseTokenizer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Utilities for working with the local dataset cache.\n",
    "This file is adapted from the AllenNLP library at https://github.com/allenai/allennlp\n",
    "Copyright by the AllenNLP authors.\n",
    "\"\"\"\n",
    "\n",
    "import fnmatch\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "import tempfile\n",
    "from contextlib import contextmanager\n",
    "from functools import partial, wraps\n",
    "from hashlib import sha256\n",
    "from typing import Optional\n",
    "from urllib.parse import urlparse\n",
    "from zipfile import ZipFile, is_zipfile\n",
    "\n",
    "import boto3\n",
    "import requests\n",
    "from botocore.config import Config\n",
    "from botocore.exceptions import ClientError\n",
    "from filelock import FileLock\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "__version__ = \"2.8.0\"\n",
    "\n",
    "logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n",
    "\n",
    "try:\n",
    "    USE_TF = os.environ.get(\"USE_TF\", \"AUTO\").upper()\n",
    "    USE_TORCH = os.environ.get(\"USE_TORCH\", \"AUTO\").upper()\n",
    "    if USE_TORCH in (\"1\", \"ON\", \"YES\", \"AUTO\") and USE_TF not in (\"1\", \"ON\", \"YES\"):\n",
    "        import torch\n",
    "\n",
    "        _torch_available = True  # pylint: disable=invalid-name\n",
    "        logger.info(\"PyTorch version {} available.\".format(torch.__version__))\n",
    "    else:\n",
    "        logger.info(\"Disabling PyTorch because USE_TF is set\")\n",
    "        _torch_available = False\n",
    "except ImportError:\n",
    "    _torch_available = False  # pylint: disable=invalid-name\n",
    "\n",
    "try:\n",
    "    USE_TF = os.environ.get(\"USE_TF\", \"AUTO\").upper()\n",
    "    USE_TORCH = os.environ.get(\"USE_TORCH\", \"AUTO\").upper()\n",
    "\n",
    "    if USE_TF in (\"1\", \"ON\", \"YES\", \"AUTO\") and USE_TORCH not in (\"1\", \"ON\", \"YES\"):\n",
    "        import tensorflow as tf\n",
    "\n",
    "        assert hasattr(tf, \"__version__\") and int(tf.__version__[0]) >= 2\n",
    "        _tf_available = True  # pylint: disable=invalid-name\n",
    "        logger.info(\"TensorFlow version {} available.\".format(tf.__version__))\n",
    "    else:\n",
    "        logger.info(\"Disabling Tensorflow because USE_TORCH is set\")\n",
    "        _tf_available = False\n",
    "except (ImportError, AssertionError):\n",
    "    _tf_available = False  # pylint: disable=invalid-name\n",
    "\n",
    "try:\n",
    "    from torch.hub import _get_torch_home\n",
    "\n",
    "    torch_cache_home = _get_torch_home()\n",
    "except ImportError:\n",
    "    torch_cache_home = os.path.expanduser(\n",
    "        os.getenv(\"TORCH_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"torch\"))\n",
    "    )\n",
    "default_cache_path = os.path.join(torch_cache_home, \"transformers\")\n",
    "\n",
    "try:\n",
    "    from pathlib import Path\n",
    "\n",
    "    PYTORCH_PRETRAINED_BERT_CACHE = Path(\n",
    "        os.getenv(\"PYTORCH_TRANSFORMERS_CACHE\", os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", default_cache_path))\n",
    "    )\n",
    "except (AttributeError, ImportError):\n",
    "    PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\n",
    "        \"PYTORCH_TRANSFORMERS_CACHE\", os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", default_cache_path)\n",
    "    )\n",
    "\n",
    "PYTORCH_TRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE  # Kept for backward compatibility\n",
    "TRANSFORMERS_CACHE = PYTORCH_PRETRAINED_BERT_CACHE  # Kept for backward compatibility\n",
    "\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
    "TF2_WEIGHTS_NAME = \"tf_model.h5\"\n",
    "TF_WEIGHTS_NAME = \"model.ckpt\"\n",
    "CONFIG_NAME = \"config.json\"\n",
    "MODEL_CARD_NAME = \"modelcard.json\"\n",
    "\n",
    "\n",
    "MULTIPLE_CHOICE_DUMMY_INPUTS = [[[0], [1]], [[0], [1]]]\n",
    "DUMMY_INPUTS = [[7, 6, 0, 0, 1], [1, 2, 3, 0, 0], [0, 0, 0, 4, 5]]\n",
    "DUMMY_MASK = [[1, 1, 1, 1, 1], [1, 1, 1, 0, 0], [0, 0, 0, 1, 1]]\n",
    "\n",
    "S3_BUCKET_PREFIX = \"https://s3.amazonaws.com/models.huggingface.co/bert\"\n",
    "CLOUDFRONT_DISTRIB_PREFIX = \"https://d2ws9o8vfrpkyk.cloudfront.net\"\n",
    "\n",
    "\n",
    "def is_torch_available():\n",
    "    return _torch_available\n",
    "\n",
    "\n",
    "def is_tf_available():\n",
    "    return _tf_available\n",
    "\n",
    "\n",
    "def add_start_docstrings(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        fn.__doc__ = \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def add_start_docstrings_to_callable(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        class_name = \":class:`~transformers.{}`\".format(fn.__qualname__.split(\".\")[0])\n",
    "        intro = \"   The {} forward method, overrides the :func:`__call__` special method.\".format(class_name)\n",
    "        note = r\"\"\"\n",
    "\n",
    "    .. note::\n",
    "        Although the recipe for forward pass needs to be defined within\n",
    "        this function, one should call the :class:`Module` instance afterwards\n",
    "        instead of this since the former takes care of running the\n",
    "        pre and post processing steps while the latter silently ignores them.\n",
    "        \"\"\"\n",
    "        fn.__doc__ = intro + note + \"\".join(docstr) + (fn.__doc__ if fn.__doc__ is not None else \"\")\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def add_end_docstrings(*docstr):\n",
    "    def docstring_decorator(fn):\n",
    "        fn.__doc__ = fn.__doc__ + \"\".join(docstr)\n",
    "        return fn\n",
    "\n",
    "    return docstring_decorator\n",
    "\n",
    "\n",
    "def is_remote_url(url_or_filename):\n",
    "    parsed = urlparse(url_or_filename)\n",
    "    return parsed.scheme in (\"http\", \"https\", \"s3\")\n",
    "\n",
    "\n",
    "def hf_bucket_url(identifier, postfix=None, cdn=False) -> str:\n",
    "    endpoint = CLOUDFRONT_DISTRIB_PREFIX if cdn else S3_BUCKET_PREFIX\n",
    "    if postfix is None:\n",
    "        return \"/\".join((endpoint, identifier))\n",
    "    else:\n",
    "        return \"/\".join((endpoint, identifier, postfix))\n",
    "\n",
    "\n",
    "def url_to_filename(url, etag=None):\n",
    "    \"\"\"\n",
    "    Convert `url` into a hashed filename in a repeatable way.\n",
    "    If `etag` is specified, append its hash to the url's, delimited\n",
    "    by a period.\n",
    "    If the url ends with .h5 (Keras HDF5 weights) adds '.h5' to the name\n",
    "    so that TF 2.0 can identify it as a HDF5 file\n",
    "    (see https://github.com/tensorflow/tensorflow/blob/00fad90125b18b80fe054de1055770cfb8fe4ba3/tensorflow/python/keras/engine/network.py#L1380)\n",
    "    \"\"\"\n",
    "    url_bytes = url.encode(\"utf-8\")\n",
    "    url_hash = sha256(url_bytes)\n",
    "    filename = url_hash.hexdigest()\n",
    "\n",
    "    if etag:\n",
    "        etag_bytes = etag.encode(\"utf-8\")\n",
    "        etag_hash = sha256(etag_bytes)\n",
    "        filename += \".\" + etag_hash.hexdigest()\n",
    "\n",
    "    if url.endswith(\".h5\"):\n",
    "        filename += \".h5\"\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "def filename_to_url(filename, cache_dir=None):\n",
    "    \"\"\"\n",
    "    Return the url and etag (which may be ``None``) stored for `filename`.\n",
    "    Raise ``EnvironmentError`` if `filename` or its stored metadata do not exist.\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "    if not os.path.exists(cache_path):\n",
    "        raise EnvironmentError(\"file {} not found\".format(cache_path))\n",
    "\n",
    "    meta_path = cache_path + \".json\"\n",
    "    if not os.path.exists(meta_path):\n",
    "        raise EnvironmentError(\"file {} not found\".format(meta_path))\n",
    "\n",
    "    with open(meta_path, encoding=\"utf-8\") as meta_file:\n",
    "        metadata = json.load(meta_file)\n",
    "    url = metadata[\"url\"]\n",
    "    etag = metadata[\"etag\"]\n",
    "\n",
    "    return url, etag\n",
    "\n",
    "\n",
    "def cached_path(\n",
    "    url_or_filename,\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    "    proxies=None,\n",
    "    resume_download=False,\n",
    "    user_agent=None,\n",
    "    extract_compressed_file=False,\n",
    "    force_extract=False,\n",
    "    local_files_only=False,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given something that might be a URL (or might be a local path),\n",
    "    determine which. If it's a URL, download the file and cache it, and\n",
    "    return the path to the cached file. If it's already a local path,\n",
    "    make sure the file exists and then return the path.\n",
    "    Args:\n",
    "        cache_dir: specify a cache directory to save the file to (overwrite the default cache dir).\n",
    "        force_download: if True, re-dowload the file even if it's already cached in the cache dir.\n",
    "        resume_download: if True, resume the download if incompletly recieved file is found.\n",
    "        user_agent: Optional string or dict that will be appended to the user-agent on remote requests.\n",
    "        extract_compressed_file: if True and the path point to a zip or tar file, extract the compressed\n",
    "            file in a folder along the archive.\n",
    "        force_extract: if True when extract_compressed_file is True and the archive was already extracted,\n",
    "            re-extract the archive and overide the folder where it was extracted.\n",
    "\n",
    "    Return:\n",
    "        None in case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n",
    "        Local path (string) otherwise\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(url_or_filename, Path):\n",
    "        url_or_filename = str(url_or_filename)\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    if is_remote_url(url_or_filename):\n",
    "        # URL, so get it from the cache (downloading if necessary)\n",
    "        output_path = get_from_cache(\n",
    "            url_or_filename,\n",
    "            cache_dir=cache_dir,\n",
    "            force_download=force_download,\n",
    "            proxies=proxies,\n",
    "            resume_download=resume_download,\n",
    "            user_agent=user_agent,\n",
    "            local_files_only=local_files_only,\n",
    "        )\n",
    "    elif os.path.exists(url_or_filename):\n",
    "        # File, and it exists.\n",
    "        output_path = url_or_filename\n",
    "    elif urlparse(url_or_filename).scheme == \"\":\n",
    "        # File, but it doesn't exist.\n",
    "        raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n",
    "    else:\n",
    "        # Something unknown\n",
    "        raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n",
    "\n",
    "    if extract_compressed_file:\n",
    "        if not is_zipfile(output_path) and not tarfile.is_tarfile(output_path):\n",
    "            return output_path\n",
    "\n",
    "        # Path where we extract compressed archives\n",
    "        # We avoid '.' in dir name and add \"-extracted\" at the end: \"./model.zip\" => \"./model-zip-extracted/\"\n",
    "        output_dir, output_file = os.path.split(output_path)\n",
    "        output_extract_dir_name = output_file.replace(\".\", \"-\") + \"-extracted\"\n",
    "        output_path_extracted = os.path.join(output_dir, output_extract_dir_name)\n",
    "\n",
    "        if os.path.isdir(output_path_extracted) and os.listdir(output_path_extracted) and not force_extract:\n",
    "            return output_path_extracted\n",
    "\n",
    "        # Prevent parallel extractions\n",
    "        lock_path = output_path + \".lock\"\n",
    "        with FileLock(lock_path):\n",
    "            shutil.rmtree(output_path_extracted, ignore_errors=True)\n",
    "            os.makedirs(output_path_extracted)\n",
    "            if is_zipfile(output_path):\n",
    "                with ZipFile(output_path, \"r\") as zip_file:\n",
    "                    zip_file.extractall(output_path_extracted)\n",
    "                    zip_file.close()\n",
    "            elif tarfile.is_tarfile(output_path):\n",
    "                tar_file = tarfile.open(output_path)\n",
    "                tar_file.extractall(output_path_extracted)\n",
    "                tar_file.close()\n",
    "            else:\n",
    "                raise EnvironmentError(\"Archive format of {} could not be identified\".format(output_path))\n",
    "\n",
    "        return output_path_extracted\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def split_s3_path(url):\n",
    "    \"\"\"Split a full s3 path into the bucket name and path.\"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    if not parsed.netloc or not parsed.path:\n",
    "        raise ValueError(\"bad s3 path {}\".format(url))\n",
    "    bucket_name = parsed.netloc\n",
    "    s3_path = parsed.path\n",
    "    # Remove '/' at beginning of path.\n",
    "    if s3_path.startswith(\"/\"):\n",
    "        s3_path = s3_path[1:]\n",
    "    return bucket_name, s3_path\n",
    "\n",
    "\n",
    "def s3_request(func):\n",
    "    \"\"\"\n",
    "    Wrapper function for s3 requests in order to create more helpful error\n",
    "    messages.\n",
    "    \"\"\"\n",
    "\n",
    "    @wraps(func)\n",
    "    def wrapper(url, *args, **kwargs):\n",
    "        try:\n",
    "            return func(url, *args, **kwargs)\n",
    "        except ClientError as exc:\n",
    "            if int(exc.response[\"Error\"][\"Code\"]) == 404:\n",
    "                raise EnvironmentError(\"file {} not found\".format(url))\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "@s3_request\n",
    "def s3_etag(url, proxies=None):\n",
    "    \"\"\"Check ETag on S3 object.\"\"\"\n",
    "    s3_resource = boto3.resource(\"s3\", config=Config(proxies=proxies))\n",
    "    bucket_name, s3_path = split_s3_path(url)\n",
    "    s3_object = s3_resource.Object(bucket_name, s3_path)\n",
    "    return s3_object.e_tag\n",
    "\n",
    "\n",
    "@s3_request\n",
    "def s3_get(url, temp_file, proxies=None):\n",
    "    \"\"\"Pull a file directly from S3.\"\"\"\n",
    "    s3_resource = boto3.resource(\"s3\", config=Config(proxies=proxies))\n",
    "    bucket_name, s3_path = split_s3_path(url)\n",
    "    s3_resource.Bucket(bucket_name).download_fileobj(s3_path, temp_file)\n",
    "\n",
    "\n",
    "def http_get(url, temp_file, proxies=None, resume_size=0, user_agent=None):\n",
    "    ua = \"transformers/{}; python/{}\".format(__version__, sys.version.split()[0])\n",
    "    if is_torch_available():\n",
    "        ua += \"; torch/{}\".format(torch.__version__)\n",
    "    if is_tf_available():\n",
    "        ua += \"; tensorflow/{}\".format(tf.__version__)\n",
    "    if isinstance(user_agent, dict):\n",
    "        ua += \"; \" + \"; \".join(\"{}/{}\".format(k, v) for k, v in user_agent.items())\n",
    "    elif isinstance(user_agent, str):\n",
    "        ua += \"; \" + user_agent\n",
    "    headers = {\"user-agent\": ua}\n",
    "    if resume_size > 0:\n",
    "        headers[\"Range\"] = \"bytes=%d-\" % (resume_size,)\n",
    "    response = requests.get(url, stream=True, proxies=proxies, headers=headers)\n",
    "    if response.status_code == 416:  # Range not satisfiable\n",
    "        return\n",
    "    content_length = response.headers.get(\"Content-Length\")\n",
    "    total = resume_size + int(content_length) if content_length is not None else None\n",
    "    progress = tqdm(\n",
    "        unit=\"B\",\n",
    "        unit_scale=True,\n",
    "        total=total,\n",
    "        initial=resume_size,\n",
    "        desc=\"Downloading\",\n",
    "        disable=bool(logger.getEffectiveLevel() == logging.NOTSET),\n",
    "    )\n",
    "    for chunk in response.iter_content(chunk_size=1024):\n",
    "        if chunk:  # filter out keep-alive new chunks\n",
    "            progress.update(len(chunk))\n",
    "            temp_file.write(chunk)\n",
    "    progress.close()\n",
    "\n",
    "\n",
    "def get_from_cache(\n",
    "    url,\n",
    "    cache_dir=None,\n",
    "    force_download=False,\n",
    "    proxies=None,\n",
    "    etag_timeout=10,\n",
    "    resume_download=False,\n",
    "    user_agent=None,\n",
    "    local_files_only=False,\n",
    ") -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Given a URL, look for the corresponding file in the local cache.\n",
    "    If it's not there, download it. Then return the path to the cached file.\n",
    "\n",
    "    Return:\n",
    "        None in case of non-recoverable file (non-existent or inaccessible url + no cache on disk).\n",
    "        Local path (string) otherwise\n",
    "    \"\"\"\n",
    "    if cache_dir is None:\n",
    "        cache_dir = TRANSFORMERS_CACHE\n",
    "    if isinstance(cache_dir, Path):\n",
    "        cache_dir = str(cache_dir)\n",
    "\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    etag = None\n",
    "    if not local_files_only:\n",
    "        # Get eTag to add to filename, if it exists.\n",
    "        if url.startswith(\"s3://\"):\n",
    "            etag = s3_etag(url, proxies=proxies)\n",
    "        else:\n",
    "            try:\n",
    "                response = requests.head(url, allow_redirects=True, proxies=proxies, timeout=etag_timeout)\n",
    "                if response.status_code == 200:\n",
    "                    etag = response.headers.get(\"ETag\")\n",
    "            except (EnvironmentError, requests.exceptions.Timeout):\n",
    "                # etag is already None\n",
    "                pass\n",
    "\n",
    "    filename = url_to_filename(url, etag)\n",
    "\n",
    "    # get cache path to put the file\n",
    "    cache_path = os.path.join(cache_dir, filename)\n",
    "\n",
    "    # etag is None = we don't have a connection, or url doesn't exist, or is otherwise inaccessible.\n",
    "    # try to get the last downloaded one\n",
    "    if etag is None:\n",
    "        if os.path.exists(cache_path):\n",
    "            return cache_path\n",
    "        else:\n",
    "            matching_files = [\n",
    "                file\n",
    "                for file in fnmatch.filter(os.listdir(cache_dir), filename + \".*\")\n",
    "                if not file.endswith(\".json\") and not file.endswith(\".lock\")\n",
    "            ]\n",
    "            if len(matching_files) > 0:\n",
    "                return os.path.join(cache_dir, matching_files[-1])\n",
    "            else:\n",
    "                # If files cannot be found and local_files_only=True,\n",
    "                # the models might've been found if local_files_only=False\n",
    "                # Notify the user about that\n",
    "                if local_files_only:\n",
    "                    raise ValueError(\n",
    "                        \"Cannot find the requested files in the cached path and outgoing traffic has been\"\n",
    "                        \" disabled. To enable model look-ups and downloads online, set 'local_files_only'\"\n",
    "                        \" to False.\"\n",
    "                    )\n",
    "                return None\n",
    "\n",
    "    # From now on, etag is not None.\n",
    "    if os.path.exists(cache_path) and not force_download:\n",
    "        return cache_path\n",
    "\n",
    "    # Prevent parallel downloads of the same file with a lock.\n",
    "    lock_path = cache_path + \".lock\"\n",
    "    with FileLock(lock_path):\n",
    "\n",
    "        if resume_download:\n",
    "            incomplete_path = cache_path + \".incomplete\"\n",
    "\n",
    "            @contextmanager\n",
    "            def _resumable_file_manager():\n",
    "                with open(incomplete_path, \"a+b\") as f:\n",
    "                    yield f\n",
    "\n",
    "            temp_file_manager = _resumable_file_manager\n",
    "            if os.path.exists(incomplete_path):\n",
    "                resume_size = os.stat(incomplete_path).st_size\n",
    "            else:\n",
    "                resume_size = 0\n",
    "        else:\n",
    "            temp_file_manager = partial(tempfile.NamedTemporaryFile, dir=cache_dir, delete=False)\n",
    "            resume_size = 0\n",
    "\n",
    "        # Download to temporary file, then copy to cache dir once finished.\n",
    "        # Otherwise you get corrupt cache entries if the download gets interrupted.\n",
    "        with temp_file_manager() as temp_file:\n",
    "            logger.info(\"%s not found in cache or force_download set to True, downloading to %s\", url, temp_file.name)\n",
    "\n",
    "            # GET file object\n",
    "            if url.startswith(\"s3://\"):\n",
    "                if resume_download:\n",
    "                    logger.warn('Warning: resumable downloads are not implemented for \"s3://\" urls')\n",
    "                s3_get(url, temp_file, proxies=proxies)\n",
    "            else:\n",
    "                http_get(url, temp_file, proxies=proxies, resume_size=resume_size, user_agent=user_agent)\n",
    "\n",
    "        logger.info(\"storing %s in cache at %s\", url, cache_path)\n",
    "        os.rename(temp_file.name, cache_path)\n",
    "\n",
    "        logger.info(\"creating metadata file for %s\", cache_path)\n",
    "        meta = {\"url\": url, \"etag\": etag}\n",
    "        meta_path = cache_path + \".json\"\n",
    "        with open(meta_path, \"w\") as meta_file:\n",
    "            json.dump(meta, meta_file)\n",
    "\n",
    "    return cache_path\n",
    "\n",
    "\n",
    "if is_tf_available():\n",
    "    import tensorflow as tf\n",
    "if is_torch_available():\n",
    "    import torch\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "SPECIAL_TOKENS_MAP_FILE = \"special_tokens_map.json\"\n",
    "ADDED_TOKENS_FILE = \"added_tokens.json\"\n",
    "TOKENIZER_CONFIG_FILE = \"tokenizer_config.json\"\n",
    "\n",
    "\n",
    "@contextmanager\n",
    "def truncate_and_pad(\n",
    "    tokenizer: BaseTokenizer,\n",
    "    max_length: int,\n",
    "    stride: int,\n",
    "    strategy: str,\n",
    "    pad_to_max_length: bool,\n",
    "    padding_side: str,\n",
    "    pad_token_id: int,\n",
    "    pad_token_type_id: int,\n",
    "    pad_token: str,\n",
    "):\n",
    "    \"\"\"\n",
    "    This contextmanager is in charge of defining the truncation and the padding strategies and then\n",
    "    restore the tokenizer settings afterwards.\n",
    "\n",
    "    This contextmanager assumes the provider tokenizer has no padding / truncation strategy\n",
    "    before the managed section. If your tokenizer set a padding / truncation strategy before,\n",
    "    then it will be reset to no padding/truncation when exiting the managed section.\n",
    "\n",
    "    :param tokenizer:\n",
    "    :param max_length:\n",
    "    :param stride:\n",
    "    :param strategy:\n",
    "    :param pad_to_max_length:\n",
    "    :param padding_side:\n",
    "    :param pad_token_id:\n",
    "    :param pad_token_type_id:\n",
    "    :param pad_token:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Handle all the truncation and padding stuff\n",
    "    if max_length is not None:\n",
    "        tokenizer.enable_truncation(max_length, stride=stride, strategy=strategy)\n",
    "\n",
    "    if pad_to_max_length and (pad_token and pad_token_id >= 0):\n",
    "        tokenizer.enable_padding(\n",
    "            max_length=max_length,\n",
    "            direction=padding_side,\n",
    "            pad_id=pad_token_id,\n",
    "            pad_type_id=pad_token_type_id,\n",
    "            pad_token=pad_token,\n",
    "        )\n",
    "    elif pad_to_max_length:\n",
    "        logger.warning(\n",
    "            \"Disabled padding because no padding token set (pad_token: {}, pad_token_id: {}).\\n\"\n",
    "            \"To remove this error, you can add a new pad token and then resize model embedding:\\n\"\n",
    "            \"\\ttokenizer.pad_token = '<PAD>'\\n\\tmodel.resize_token_embeddings(len(tokenizer))\".format(\n",
    "                pad_token, pad_token_id\n",
    "            )\n",
    "        )\n",
    "\n",
    "    yield\n",
    "\n",
    "    if max_length is not None:\n",
    "        tokenizer.no_truncation()\n",
    "\n",
    "    if pad_to_max_length and (pad_token and pad_token_id >= 0):\n",
    "        tokenizer.no_padding()\n",
    "\n",
    "\n",
    "class PreTrainedTokenizer(object):\n",
    "    \"\"\" Base class for all tokenizers.\n",
    "    Handle all the shared methods for tokenization and special tokens as well as methods downloading/caching/loading pretrained tokenizers as well as adding tokens to the vocabulary.\n",
    "\n",
    "    This class also contain the added tokens in a unified way on top of all tokenizers so we don't have to handle the specific vocabulary augmentation methods of the various underlying dictionary structures (BPE, sentencepiece...).\n",
    "\n",
    "    Class attributes (overridden by derived classes):\n",
    "\n",
    "        - ``vocab_files_names``: a python ``dict`` with, as keys, the ``__init__`` keyword name of each vocabulary file required by the model, and as associated values, the filename for saving the associated file (string).\n",
    "        - ``pretrained_vocab_files_map``: a python ``dict of dict`` the high-level keys being the ``__init__`` keyword name of each vocabulary file required by the model, the low-level being the `short-cut-names` (string) of the pretrained models with, as associated values, the `url` (string) to the associated pretrained vocabulary file.\n",
    "        - ``max_model_input_sizes``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, the maximum length of the sequence inputs of this model, or None if the model has no maximum input size.\n",
    "        - ``pretrained_init_configuration``: a python ``dict`` with, as keys, the `short-cut-names` (string) of the pretrained models, and as associated values, a dictionnary of specific arguments to pass to the ``__init__``method of the tokenizer class for this pretrained model when loading the tokenizer with the ``from_pretrained()`` method.\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        - ``bos_token``: (`Optional`) string: a beginning of sentence token. Will be associated to ``self.bos_token`` and ``self.bos_token_id``\n",
    "\n",
    "        - ``eos_token``: (`Optional`) string: an end of sentence token. Will be associated to ``self.eos_token`` and ``self.eos_token_id``\n",
    "\n",
    "        - ``unk_token``: (`Optional`) string: an unknown token. Will be associated to ``self.unk_token`` and ``self.unk_token_id``\n",
    "\n",
    "        - ``sep_token``: (`Optional`) string: a separation token (e.g. to separate context and query in an input sequence). Will be associated to ``self.sep_token`` and ``self.sep_token_id``\n",
    "\n",
    "        - ``pad_token``: (`Optional`) string: a padding token. Will be associated to ``self.pad_token`` and ``self.pad_token_id``\n",
    "\n",
    "        - ``cls_token``: (`Optional`) string: a classification token (e.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model). Will be associated to ``self.cls_token`` and ``self.cls_token_id``\n",
    "\n",
    "        - ``mask_token``: (`Optional`) string: a masking token (e.g. when training a model with masked-language modeling). Will be associated to ``self.mask_token`` and ``self.mask_token_id``\n",
    "\n",
    "        - ``additional_special_tokens``: (`Optional`) list: a list of additional special tokens. Adding all special tokens here ensure they won't be split by the tokenization process. Will be associated to ``self.additional_special_tokens`` and ``self.additional_special_tokens_ids``\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = {}\n",
    "    pretrained_vocab_files_map = {}\n",
    "    pretrained_init_configuration = {}\n",
    "    max_model_input_sizes = {}\n",
    "    model_input_names = [\"token_type_ids\", \"attention_mask\"]\n",
    "\n",
    "    SPECIAL_TOKENS_ATTRIBUTES = [\n",
    "        \"bos_token\",\n",
    "        \"eos_token\",\n",
    "        \"unk_token\",\n",
    "        \"sep_token\",\n",
    "        \"pad_token\",\n",
    "        \"cls_token\",\n",
    "        \"mask_token\",\n",
    "        \"additional_special_tokens\",\n",
    "    ]\n",
    "\n",
    "    padding_side = \"right\"\n",
    "\n",
    "    NO_PAD_TOKEN_FOR_BATCH_MSG = (\n",
    "        \"No padding token is set for this model, therefore no batch can be made with uneven \"\n",
    "        \"sequences. Set a padding token or adjust the lengths of the sequences building the \"\n",
    "        \"batch so that every sequence is of the same length.\"\n",
    "    )\n",
    "\n",
    "    UNEVEN_SEQUENCES_FOR_BATCH_MSG = (\n",
    "        \"The sequences building the batch are not of the same size, no tensor \"\n",
    "        \"can be built. Set `pad_to_max_length=True` to pad the smaller sequences\"\n",
    "        \"up to the larger sequence's length.\"\n",
    "    )\n",
    "\n",
    "    @property\n",
    "    def bos_token(self):\n",
    "        \"\"\" Beginning of sentence token (string). Log an error if used while not having been set. \"\"\"\n",
    "        if self._bos_token is None:\n",
    "            logger.error(\"Using bos_token, but it is not set yet.\")\n",
    "        return self._bos_token\n",
    "\n",
    "    @property\n",
    "    def eos_token(self):\n",
    "        \"\"\" End of sentence token (string). Log an error if used while not having been set. \"\"\"\n",
    "        if self._eos_token is None:\n",
    "            logger.error(\"Using eos_token, but it is not set yet.\")\n",
    "        return self._eos_token\n",
    "\n",
    "    @property\n",
    "    def unk_token(self):\n",
    "        \"\"\" Unknown token (string). Log an error if used while not having been set. \"\"\"\n",
    "        if self._unk_token is None:\n",
    "            logger.error(\"Using unk_token, but it is not set yet.\")\n",
    "        return self._unk_token\n",
    "\n",
    "    @property\n",
    "    def sep_token(self):\n",
    "        \"\"\" Separation token (string). E.g. separate context and query in an input sequence. Log an error if used while not having been set. \"\"\"\n",
    "        if self._sep_token is None:\n",
    "            logger.error(\"Using sep_token, but it is not set yet.\")\n",
    "        return self._sep_token\n",
    "\n",
    "    @property\n",
    "    def pad_token(self):\n",
    "        \"\"\" Padding token (string). Log an error if used while not having been set. \"\"\"\n",
    "        if self._pad_token is None:\n",
    "            logger.error(\"Using pad_token, but it is not set yet.\")\n",
    "        return self._pad_token\n",
    "\n",
    "    @property\n",
    "    def cls_token(self):\n",
    "        \"\"\" Classification token (string). E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. \"\"\"\n",
    "        if self._cls_token is None:\n",
    "            logger.error(\"Using cls_token, but it is not set yet.\")\n",
    "        return self._cls_token\n",
    "\n",
    "    @property\n",
    "    def mask_token(self):\n",
    "        \"\"\" Mask token (string). E.g. when training a model with masked-language modeling. Log an error if used while not having been set. \"\"\"\n",
    "        if self._mask_token is None:\n",
    "            logger.error(\"Using mask_token, but it is not set yet.\")\n",
    "        return self._mask_token\n",
    "\n",
    "    @property\n",
    "    def additional_special_tokens(self):\n",
    "        \"\"\" All the additional special tokens you may want to use (list of strings). Log an error if used while not having been set. \"\"\"\n",
    "        if self._additional_special_tokens is None:\n",
    "            logger.error(\"Using additional_special_tokens, but it is not set yet.\")\n",
    "        return self._additional_special_tokens\n",
    "\n",
    "    @bos_token.setter\n",
    "    def bos_token(self, value):\n",
    "        self._bos_token = value\n",
    "\n",
    "    @eos_token.setter\n",
    "    def eos_token(self, value):\n",
    "        self._eos_token = value\n",
    "\n",
    "    @unk_token.setter\n",
    "    def unk_token(self, value):\n",
    "        self._unk_token = value\n",
    "\n",
    "    @sep_token.setter\n",
    "    def sep_token(self, value):\n",
    "        self._sep_token = value\n",
    "\n",
    "    @pad_token.setter\n",
    "    def pad_token(self, value):\n",
    "        self._pad_token = value\n",
    "\n",
    "    @cls_token.setter\n",
    "    def cls_token(self, value):\n",
    "        self._cls_token = value\n",
    "\n",
    "    @mask_token.setter\n",
    "    def mask_token(self, value):\n",
    "        self._mask_token = value\n",
    "\n",
    "    @additional_special_tokens.setter\n",
    "    def additional_special_tokens(self, value):\n",
    "        self._additional_special_tokens = value\n",
    "\n",
    "    @property\n",
    "    def bos_token_id(self):\n",
    "        \"\"\" Id of the beginning of sentence token in the vocabulary. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.bos_token)\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self):\n",
    "        \"\"\" Id of the end of sentence token in the vocabulary. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.eos_token)\n",
    "\n",
    "    @property\n",
    "    def unk_token_id(self):\n",
    "        \"\"\" Id of the unknown token in the vocabulary. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.unk_token)\n",
    "\n",
    "    @property\n",
    "    def sep_token_id(self):\n",
    "        \"\"\" Id of the separation token in the vocabulary. E.g. separate context and query in an input sequence. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.sep_token)\n",
    "\n",
    "    @property\n",
    "    def pad_token_id(self):\n",
    "        \"\"\" Id of the padding token in the vocabulary. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.pad_token)\n",
    "\n",
    "    @property\n",
    "    def pad_token_type_id(self):\n",
    "        \"\"\" Id of the padding token type in the vocabulary.\"\"\"\n",
    "        return self._pad_token_type_id\n",
    "\n",
    "    @property\n",
    "    def cls_token_id(self):\n",
    "        \"\"\" Id of the classification token in the vocabulary. E.g. to extract a summary of an input sequence leveraging self-attention along the full depth of the model. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.cls_token)\n",
    "\n",
    "    @property\n",
    "    def mask_token_id(self):\n",
    "        \"\"\" Id of the mask token in the vocabulary. E.g. when training a model with masked-language modeling. Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.mask_token)\n",
    "\n",
    "    @property\n",
    "    def additional_special_tokens_ids(self):\n",
    "        \"\"\" Ids of all the additional special tokens in the vocabulary (list of integers). Log an error if used while not having been set. \"\"\"\n",
    "        return self.convert_tokens_to_ids(self.additional_special_tokens)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        \"\"\" Returns the vocabulary as a dict of {token: index} pairs. `tokenizer.get_vocab()[token]` is equivalent to `tokenizer.convert_tokens_to_ids(token)` when `token` is in the vocab. \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __init__(self, max_len=None, **kwargs):\n",
    "        self._bos_token = None\n",
    "        self._eos_token = None\n",
    "        self._unk_token = None\n",
    "        self._sep_token = None\n",
    "        self._pad_token = None\n",
    "        self._cls_token = None\n",
    "        self._mask_token = None\n",
    "        self._pad_token_type_id = 0\n",
    "        self._additional_special_tokens = []\n",
    "\n",
    "        self.max_len = max_len if max_len is not None else int(1e12)\n",
    "\n",
    "        # Padding side is right by default and over-riden in subclasses. If specified in the kwargs, it is changed.\n",
    "        self.padding_side = kwargs.pop(\"padding_side\", self.padding_side)\n",
    "        self.model_input_names = kwargs.pop(\"model_input_names\", self.model_input_names)\n",
    "\n",
    "        # Added tokens\n",
    "        self.added_tokens_encoder = {}\n",
    "        self.unique_added_tokens_encoder = set()\n",
    "        self.added_tokens_decoder = {}\n",
    "\n",
    "        # inputs and kwargs for saving and re-loading (see ``from_pretrained`` and ``save_pretrained``)\n",
    "        self.init_inputs = ()\n",
    "        self.init_kwargs = {}\n",
    "\n",
    "        for key, value in kwargs.items():\n",
    "            if key in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
    "                if key == \"additional_special_tokens\":\n",
    "                    assert isinstance(value, (list, tuple)) and all(isinstance(t, str) for t in value)\n",
    "                else:\n",
    "                    assert isinstance(value, str)\n",
    "                setattr(self, key, value)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, *inputs, **kwargs):\n",
    "        r\"\"\"\n",
    "        Instantiate a :class:`~transformers.PreTrainedTokenizer` (or a derived class) from a predefined tokenizer.\n",
    "\n",
    "        Args:\n",
    "            pretrained_model_name_or_path: either:\n",
    "\n",
    "                - a string with the `shortcut name` of a predefined tokenizer to load from cache or download, e.g.: ``bert-base-uncased``.\n",
    "                - a string with the `identifier name` of a predefined tokenizer that was user-uploaded to our S3, e.g.: ``dbmdz/bert-base-german-cased``.\n",
    "                - a path to a `directory` containing vocabulary files required by the tokenizer, for instance saved using the :func:`~transformers.PreTrainedTokenizer.save_pretrained` method, e.g.: ``./my_model_directory/``.\n",
    "                - (not applicable to all derived classes, deprecated) a path or url to a single saved vocabulary file if and only if the tokenizer only requires a single vocabulary file (e.g. Bert, XLNet), e.g.: ``./my_model_directory/vocab.txt``.\n",
    "\n",
    "            cache_dir: (`optional`) string:\n",
    "                Path to a directory in which a downloaded predefined tokenizer vocabulary files should be cached if the standard cache should not be used.\n",
    "\n",
    "            force_download: (`optional`) boolean, default False:\n",
    "                Force to (re-)download the vocabulary files and override the cached versions if they exists.\n",
    "\n",
    "            resume_download: (`optional`) boolean, default False:\n",
    "                Do not delete incompletely recieved file. Attempt to resume the download if such a file exists.\n",
    "\n",
    "            proxies: (`optional`) dict, default None:\n",
    "                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128', 'http://hostname': 'foo.bar:4012'}.\n",
    "                The proxies are used on each request.\n",
    "\n",
    "            inputs: (`optional`) positional arguments: will be passed to the Tokenizer ``__init__`` method.\n",
    "\n",
    "            kwargs: (`optional`) keyword arguments: will be passed to the Tokenizer ``__init__`` method. Can be used to set special tokens like ``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``, ``additional_special_tokens``. See parameters in the doc string of :class:`~transformers.PreTrainedTokenizer` for details.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # We can't instantiate directly the base class `PreTrainedTokenizer` so let's show our examples on a derived class: BertTokenizer\n",
    "\n",
    "            # Download vocabulary from S3 and cache.\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            # Download vocabulary from S3 (user-uploaded) and cache.\n",
    "            tokenizer = BertTokenizer.from_pretrained('dbmdz/bert-base-german-cased')\n",
    "\n",
    "            # If vocabulary files are in a directory (e.g. tokenizer was saved using `save_pretrained('./test/saved_model/')`)\n",
    "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/')\n",
    "\n",
    "            # If the tokenizer uses a single vocabulary file, you can point directly to this file\n",
    "            tokenizer = BertTokenizer.from_pretrained('./test/saved_model/my_vocab.txt')\n",
    "\n",
    "            # You can link tokens to special vocabulary when instantiating\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', unk_token='<unk>')\n",
    "            # You should be sure '<unk>' is in the vocabulary when doing that.\n",
    "            # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n",
    "            assert tokenizer.unk_token == '<unk>'\n",
    "\n",
    "        \"\"\"\n",
    "        return cls._from_pretrained(*inputs, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def _from_pretrained(cls, pretrained_model_name_or_path, *init_inputs, **kwargs):\n",
    "        cache_dir = kwargs.pop(\"cache_dir\", None)\n",
    "        force_download = kwargs.pop(\"force_download\", False)\n",
    "        resume_download = kwargs.pop(\"resume_download\", False)\n",
    "        proxies = kwargs.pop(\"proxies\", None)\n",
    "        local_files_only = kwargs.pop(\"local_files_only\", False)\n",
    "\n",
    "        s3_models = list(cls.max_model_input_sizes.keys())\n",
    "        vocab_files = {}\n",
    "        init_configuration = {}\n",
    "        if pretrained_model_name_or_path in s3_models:\n",
    "            # Get the vocabulary from AWS S3 bucket\n",
    "            for file_id, map_list in cls.pretrained_vocab_files_map.items():\n",
    "                vocab_files[file_id] = map_list[pretrained_model_name_or_path]\n",
    "            if (\n",
    "                cls.pretrained_init_configuration\n",
    "                and pretrained_model_name_or_path in cls.pretrained_init_configuration\n",
    "            ):\n",
    "                init_configuration = cls.pretrained_init_configuration[pretrained_model_name_or_path].copy()\n",
    "        else:\n",
    "            # Get the vocabulary from local files\n",
    "            logger.info(\n",
    "                \"Model name '{}' not found in model shortcut name list ({}). \"\n",
    "                \"Assuming '{}' is a path, a model identifier, or url to a directory containing tokenizer files.\".format(\n",
    "                    pretrained_model_name_or_path, \", \".join(s3_models), pretrained_model_name_or_path\n",
    "                )\n",
    "            )\n",
    "\n",
    "            if os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n",
    "                if len(cls.vocab_files_names) > 1:\n",
    "                    raise ValueError(\n",
    "                        \"Calling {}.from_pretrained() with the path to a single file or url is not supported.\"\n",
    "                        \"Use a model identifier or the path to a directory instead.\".format(cls.__name__)\n",
    "                    )\n",
    "                logger.warning(\n",
    "                    \"Calling {}.from_pretrained() with the path to a single file or url is deprecated\".format(\n",
    "                        cls.__name__\n",
    "                    )\n",
    "                )\n",
    "                file_id = list(cls.vocab_files_names.keys())[0]\n",
    "                vocab_files[file_id] = pretrained_model_name_or_path\n",
    "            else:\n",
    "                # At this point pretrained_model_name_or_path is either a directory or a model identifier name\n",
    "                additional_files_names = {\n",
    "                    \"added_tokens_file\": ADDED_TOKENS_FILE,\n",
    "                    \"special_tokens_map_file\": SPECIAL_TOKENS_MAP_FILE,\n",
    "                    \"tokenizer_config_file\": TOKENIZER_CONFIG_FILE,\n",
    "                }\n",
    "                # Look for the tokenizer main vocabulary files + the additional tokens files\n",
    "                for file_id, file_name in {**cls.vocab_files_names, **additional_files_names}.items():\n",
    "                    if os.path.isdir(pretrained_model_name_or_path):\n",
    "                        full_file_name = os.path.join(pretrained_model_name_or_path, file_name)\n",
    "                        if not os.path.exists(full_file_name):\n",
    "                            logger.info(\"Didn't find file {}. We won't load it.\".format(full_file_name))\n",
    "                            full_file_name = None\n",
    "                    else:\n",
    "                        full_file_name = hf_bucket_url(pretrained_model_name_or_path, postfix=file_name)\n",
    "\n",
    "                    vocab_files[file_id] = full_file_name\n",
    "\n",
    "        # Get files from url, cache, or disk depending on the case\n",
    "        try:\n",
    "            resolved_vocab_files = {}\n",
    "            for file_id, file_path in vocab_files.items():\n",
    "                if file_path is None:\n",
    "                    resolved_vocab_files[file_id] = None\n",
    "                else:\n",
    "                    resolved_vocab_files[file_id] = cached_path(\n",
    "                        file_path,\n",
    "                        cache_dir=cache_dir,\n",
    "                        force_download=force_download,\n",
    "                        proxies=proxies,\n",
    "                        resume_download=resume_download,\n",
    "                        local_files_only=local_files_only,\n",
    "                    )\n",
    "        except EnvironmentError:\n",
    "            if pretrained_model_name_or_path in s3_models:\n",
    "                msg = \"Couldn't reach server at '{}' to download vocabulary files.\"\n",
    "            else:\n",
    "                msg = (\n",
    "                    \"Model name '{}' was not found in tokenizers model name list ({}). \"\n",
    "                    \"We assumed '{}' was a path or url to a directory containing vocabulary files \"\n",
    "                    \"named {}, but couldn't find such vocabulary files at this path or url.\".format(\n",
    "                        pretrained_model_name_or_path,\n",
    "                        \", \".join(s3_models),\n",
    "                        pretrained_model_name_or_path,\n",
    "                        list(cls.vocab_files_names.values()),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            raise EnvironmentError(msg)\n",
    "\n",
    "        if all(full_file_name is None for full_file_name in resolved_vocab_files.values()):\n",
    "            raise EnvironmentError(\n",
    "                \"Model name '{}' was not found in tokenizers model name list ({}). \"\n",
    "                \"We assumed '{}' was a path, a model identifier, or url to a directory containing vocabulary files \"\n",
    "                \"named {} but couldn't find such vocabulary files at this path or url.\".format(\n",
    "                    pretrained_model_name_or_path,\n",
    "                    \", \".join(s3_models),\n",
    "                    pretrained_model_name_or_path,\n",
    "                    list(cls.vocab_files_names.values()),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        for file_id, file_path in vocab_files.items():\n",
    "            if file_path == resolved_vocab_files[file_id]:\n",
    "                logger.info(\"loading file {}\".format(file_path))\n",
    "            else:\n",
    "                logger.info(\"loading file {} from cache at {}\".format(file_path, resolved_vocab_files[file_id]))\n",
    "\n",
    "        # Prepare tokenizer initialization kwargs\n",
    "        # Did we saved some inputs and kwargs to reload ?\n",
    "        tokenizer_config_file = resolved_vocab_files.pop(\"tokenizer_config_file\", None)\n",
    "        if tokenizer_config_file is not None:\n",
    "            with open(tokenizer_config_file, encoding=\"utf-8\") as tokenizer_config_handle:\n",
    "                init_kwargs = json.load(tokenizer_config_handle)\n",
    "            saved_init_inputs = init_kwargs.pop(\"init_inputs\", ())\n",
    "            if not init_inputs:\n",
    "                init_inputs = saved_init_inputs\n",
    "        else:\n",
    "            init_kwargs = init_configuration\n",
    "\n",
    "        # Update with newly provided kwargs\n",
    "        init_kwargs.update(kwargs)\n",
    "\n",
    "        # Set max length if needed\n",
    "        if pretrained_model_name_or_path in cls.max_model_input_sizes:\n",
    "            # if we're using a pretrained model, ensure the tokenizer\n",
    "            # wont index sequences longer than the number of positional embeddings\n",
    "            max_len = cls.max_model_input_sizes[pretrained_model_name_or_path]\n",
    "            if max_len is not None and isinstance(max_len, (int, float)):\n",
    "                init_kwargs[\"max_len\"] = min(init_kwargs.get(\"max_len\", int(1e12)), max_len)\n",
    "\n",
    "        # Merge resolved_vocab_files arguments in init_kwargs.\n",
    "        added_tokens_file = resolved_vocab_files.pop(\"added_tokens_file\", None)\n",
    "        special_tokens_map_file = resolved_vocab_files.pop(\"special_tokens_map_file\", None)\n",
    "        for args_name, file_path in resolved_vocab_files.items():\n",
    "            if args_name not in init_kwargs:\n",
    "                init_kwargs[args_name] = file_path\n",
    "        if special_tokens_map_file is not None:\n",
    "            with open(special_tokens_map_file, encoding=\"utf-8\") as special_tokens_map_handle:\n",
    "                special_tokens_map = json.load(special_tokens_map_handle)\n",
    "            for key, value in special_tokens_map.items():\n",
    "                if key not in init_kwargs:\n",
    "                    init_kwargs[key] = value\n",
    "\n",
    "        # Instantiate tokenizer.\n",
    "        try:\n",
    "            tokenizer = cls(*init_inputs, **init_kwargs)\n",
    "        except OSError:\n",
    "            raise OSError(\n",
    "                \"Unable to load vocabulary from file. \"\n",
    "                \"Please check that the provided vocabulary is accessible and not corrupted.\"\n",
    "            )\n",
    "\n",
    "        # Save inputs and kwargs for saving and re-loading with ``save_pretrained``\n",
    "        tokenizer.init_inputs = init_inputs\n",
    "        tokenizer.init_kwargs = init_kwargs\n",
    "\n",
    "        # update unique_added_tokens_encoder with special tokens for correct tokenization\n",
    "        tokenizer.unique_added_tokens_encoder.update(set(tokenizer.all_special_tokens))\n",
    "\n",
    "        # Add supplementary tokens.\n",
    "        if added_tokens_file is not None:\n",
    "            with open(added_tokens_file, encoding=\"utf-8\") as added_tokens_handle:\n",
    "                added_tok_encoder = json.load(added_tokens_handle)\n",
    "            added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}\n",
    "            tokenizer.added_tokens_encoder.update(added_tok_encoder)\n",
    "            tokenizer.added_tokens_decoder.update(added_tok_decoder)\n",
    "            tokenizer.unique_added_tokens_encoder.update(set(tokenizer.added_tokens_encoder.keys()))\n",
    "\n",
    "        return tokenizer\n",
    "\n",
    "    def save_pretrained(self, save_directory):\n",
    "        \"\"\" Save the tokenizer vocabulary files together with:\n",
    "                - added tokens,\n",
    "                - special-tokens-to-class-attributes-mapping,\n",
    "                - tokenizer instantiation positional and keywords inputs (e.g. do_lower_case for Bert).\n",
    "\n",
    "            This won't save modifications other than (added tokens and special token mapping) you may have\n",
    "            applied to the tokenizer after the instantiation (e.g. modifying tokenizer.do_lower_case after creation).\n",
    "\n",
    "            This method make sure the full tokenizer can then be re-loaded using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Saving directory ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "\n",
    "        special_tokens_map_file = os.path.join(save_directory, SPECIAL_TOKENS_MAP_FILE)\n",
    "        added_tokens_file = os.path.join(save_directory, ADDED_TOKENS_FILE)\n",
    "        tokenizer_config_file = os.path.join(save_directory, TOKENIZER_CONFIG_FILE)\n",
    "\n",
    "        tokenizer_config = copy.deepcopy(self.init_kwargs)\n",
    "        if len(self.init_inputs) > 0:\n",
    "            tokenizer_config[\"init_inputs\"] = copy.deepcopy(self.init_inputs)\n",
    "        for file_id in self.vocab_files_names.keys():\n",
    "            tokenizer_config.pop(file_id, None)\n",
    "\n",
    "        with open(tokenizer_config_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(tokenizer_config, ensure_ascii=False))\n",
    "\n",
    "        with open(special_tokens_map_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(self.special_tokens_map, ensure_ascii=False))\n",
    "\n",
    "        if len(self.added_tokens_encoder) > 0:\n",
    "            with open(added_tokens_file, \"w\", encoding=\"utf-8\") as f:\n",
    "                out_str = json.dumps(self.added_tokens_encoder, ensure_ascii=False)\n",
    "                f.write(out_str)\n",
    "\n",
    "        vocab_files = self.save_vocabulary(save_directory)\n",
    "\n",
    "        return vocab_files + (special_tokens_map_file, added_tokens_file)\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\" Save the tokenizer vocabulary to a directory. This method does *NOT* save added tokens\n",
    "            and special token mappings.\n",
    "\n",
    "            Please use :func:`~transformers.PreTrainedTokenizer.save_pretrained` `()` to save the full Tokenizer state if you want to reload it using the :func:`~transformers.PreTrainedTokenizer.from_pretrained` class method.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def vocab_size(self):\n",
    "        \"\"\" Size of the base vocabulary (without the added tokens) \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Size of the full vocabulary with the added tokens \"\"\"\n",
    "        return self.vocab_size + len(self.added_tokens_encoder)\n",
    "\n",
    "    def add_tokens(self, new_tokens):\n",
    "        \"\"\"\n",
    "        Add a list of new tokens to the tokenizer class. If the new tokens are not in the\n",
    "        vocabulary, they are added to it with indices starting from length of the current vocabulary.\n",
    "\n",
    "        Args:\n",
    "            new_tokens: string or list of string. Each string is a token to add. Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
    "\n",
    "        Returns:\n",
    "            Number of tokens added to the vocabulary.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # Let's see how to increase the vocabulary of Bert model and tokenizer\n",
    "            tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "            model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "            num_added_toks = tokenizer.add_tokens(['new_tok1', 'my_new-tok2'])\n",
    "            print('We have added', num_added_toks, 'tokens')\n",
    "            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
    "        \"\"\"\n",
    "        if not new_tokens:\n",
    "            return 0\n",
    "\n",
    "        if not isinstance(new_tokens, list):\n",
    "            new_tokens = [new_tokens]\n",
    "\n",
    "        to_add_tokens = []\n",
    "        for token in new_tokens:\n",
    "            assert isinstance(token, str)\n",
    "            if self.init_kwargs.get(\"do_lower_case\", False) and token not in self.all_special_tokens:\n",
    "                token = token.lower()\n",
    "            if (\n",
    "                token != self.unk_token\n",
    "                and self.convert_tokens_to_ids(token) == self.convert_tokens_to_ids(self.unk_token)\n",
    "                and token not in to_add_tokens\n",
    "            ):\n",
    "                to_add_tokens.append(token)\n",
    "                logger.info(\"Adding %s to the vocabulary\", token)\n",
    "\n",
    "        added_tok_encoder = dict((tok, len(self) + i) for i, tok in enumerate(to_add_tokens))\n",
    "        added_tok_decoder = {v: k for k, v in added_tok_encoder.items()}\n",
    "        self.added_tokens_encoder.update(added_tok_encoder)\n",
    "        self.unique_added_tokens_encoder = set(self.added_tokens_encoder.keys()).union(set(self.all_special_tokens))\n",
    "        self.added_tokens_decoder.update(added_tok_decoder)\n",
    "\n",
    "        return len(to_add_tokens)\n",
    "\n",
    "    def num_added_tokens(self, pair=False):\n",
    "        \"\"\"\n",
    "        Returns the number of added tokens when encoding a sequence with special tokens.\n",
    "\n",
    "        Note:\n",
    "            This encodes inputs and checks the number of added tokens, and is therefore not efficient. Do not put this\n",
    "            inside your training loop.\n",
    "\n",
    "        Args:\n",
    "            pair: Returns the number of added tokens in the case of a sequence pair if set to True, returns the\n",
    "                number of added tokens in the case of a single sequence if set to False.\n",
    "\n",
    "        Returns:\n",
    "            Number of tokens added to sequences\n",
    "        \"\"\"\n",
    "        token_ids_0 = []\n",
    "        token_ids_1 = []\n",
    "        return len(self.build_inputs_with_special_tokens(token_ids_0, token_ids_1 if pair else None))\n",
    "\n",
    "    def add_special_tokens(self, special_tokens_dict):\n",
    "        \"\"\"\n",
    "        Add a dictionary of special tokens (eos, pad, cls...) to the encoder and link them\n",
    "        to class attributes. If special tokens are NOT in the vocabulary, they are added\n",
    "        to it (indexed starting from the last index of the current vocabulary).\n",
    "\n",
    "        Using `add_special_tokens` will ensure your special tokens can be used in several ways:\n",
    "\n",
    "        - special tokens are carefully handled by the tokenizer (they are never split)\n",
    "        - you can easily refer to special tokens using tokenizer class attributes like `tokenizer.cls_token`. This makes it easy to develop model-agnostic training and fine-tuning scripts.\n",
    "\n",
    "        When possible, special tokens are already registered for provided pretrained models (ex: BertTokenizer cls_token is already registered to be '[CLS]' and XLM's one is also registered to be '</s>')\n",
    "\n",
    "        Args:\n",
    "            special_tokens_dict: dict of string. Keys should be in the list of predefined special attributes:\n",
    "                [``bos_token``, ``eos_token``, ``unk_token``, ``sep_token``, ``pad_token``, ``cls_token``, ``mask_token``,\n",
    "                ``additional_special_tokens``].\n",
    "\n",
    "                Tokens are only added if they are not already in the vocabulary (tested by checking if the tokenizer assign the index of the ``unk_token`` to them).\n",
    "\n",
    "        Returns:\n",
    "            Number of tokens added to the vocabulary.\n",
    "\n",
    "        Examples::\n",
    "\n",
    "            # Let's see how to add a new classification token to GPT-2\n",
    "            tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "            model = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "            special_tokens_dict = {'cls_token': '<CLS>'}\n",
    "\n",
    "            num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "            print('We have added', num_added_toks, 'tokens')\n",
    "            model.resize_token_embeddings(len(tokenizer))  # Notice: resize_token_embeddings expect to receive the full size of the new vocabulary, i.e. the length of the tokenizer.\n",
    "\n",
    "            assert tokenizer.cls_token == '<CLS>'\n",
    "        \"\"\"\n",
    "        if not special_tokens_dict:\n",
    "            return 0\n",
    "\n",
    "        added_tokens = 0\n",
    "        for key, value in special_tokens_dict.items():\n",
    "            assert key in self.SPECIAL_TOKENS_ATTRIBUTES\n",
    "            if key == \"additional_special_tokens\":\n",
    "                assert isinstance(value, (list, tuple)) and all(isinstance(t, str) for t in value)\n",
    "                added_tokens += self.add_tokens(value)\n",
    "            else:\n",
    "                assert isinstance(value, str)\n",
    "                added_tokens += self.add_tokens([value])\n",
    "            logger.info(\"Assigning %s to the %s key of the tokenizer\", value, key)\n",
    "            setattr(self, key, value)\n",
    "\n",
    "        return added_tokens\n",
    "\n",
    "    def tokenize(self, text, **kwargs):\n",
    "        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n",
    "            Split in words for word-based vocabulary or sub-words for sub-word-based\n",
    "            vocabularies (BPE/SentencePieces/WordPieces).\n",
    "\n",
    "            Take care of added tokens.\n",
    "\n",
    "            text: The sequence to be encoded.\n",
    "            add_prefix_space: Only applies to GPT-2 and RoBERTa tokenizers. When `True`, this ensures that the sequence\n",
    "                begins with an empty space. False by default except for when using RoBERTa with `add_special_tokens=True`.\n",
    "            **kwargs: passed to the `prepare_for_tokenization` preprocessing method.\n",
    "        \"\"\"\n",
    "        all_special_tokens = self.all_special_tokens\n",
    "        text = self.prepare_for_tokenization(text, **kwargs)\n",
    "\n",
    "        def lowercase_text(t):\n",
    "            # convert non-special tokens to lowercase\n",
    "            escaped_special_toks = [re.escape(s_tok) for s_tok in all_special_tokens]\n",
    "            pattern = r\"(\" + r\"|\".join(escaped_special_toks) + r\")|\" + r\"(.+?)\"\n",
    "            return re.sub(pattern, lambda m: m.groups()[0] or m.groups()[1].lower(), t)\n",
    "\n",
    "        if self.init_kwargs.get(\"do_lower_case\", False):\n",
    "            text = lowercase_text(text)\n",
    "\n",
    "        def split_on_token(tok, text):\n",
    "            result = []\n",
    "            split_text = text.split(tok)\n",
    "            for i, sub_text in enumerate(split_text):\n",
    "                sub_text = sub_text.rstrip()\n",
    "                if i == 0 and not sub_text:\n",
    "                    result += [tok]\n",
    "                elif i == len(split_text) - 1:\n",
    "                    if sub_text:\n",
    "                        result += [sub_text]\n",
    "                    else:\n",
    "                        pass\n",
    "                else:\n",
    "                    if sub_text:\n",
    "                        result += [sub_text]\n",
    "                    result += [tok]\n",
    "            return result\n",
    "\n",
    "        def split_on_tokens(tok_list, text):\n",
    "            if not text.strip():\n",
    "                return []\n",
    "            if not tok_list:\n",
    "                return self._tokenize(text)\n",
    "\n",
    "            tokenized_text = []\n",
    "            text_list = [text]\n",
    "            for tok in tok_list:\n",
    "                tokenized_text = []\n",
    "                for sub_text in text_list:\n",
    "                    if sub_text not in self.unique_added_tokens_encoder:\n",
    "                        tokenized_text += split_on_token(tok, sub_text)\n",
    "                    else:\n",
    "                        tokenized_text += [sub_text]\n",
    "                text_list = tokenized_text\n",
    "\n",
    "            return list(\n",
    "                itertools.chain.from_iterable(\n",
    "                    (\n",
    "                        self._tokenize(token) if token not in self.unique_added_tokens_encoder else [token]\n",
    "                        for token in tokenized_text\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "\n",
    "        added_tokens = self.unique_added_tokens_encoder\n",
    "        tokenized_text = split_on_tokens(added_tokens, text)\n",
    "        return tokenized_text\n",
    "\n",
    "    def _tokenize(self, text, **kwargs):\n",
    "        \"\"\" Converts a string in a sequence of tokens (string), using the tokenizer.\n",
    "            Split in words for word-based vocabulary or sub-words for sub-word-based\n",
    "            vocabularies (BPE/SentencePieces/WordPieces).\n",
    "\n",
    "            Do NOT take care of added tokens.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\" Converts a single token, or a sequence of tokens, (str) in a single integer id\n",
    "            (resp. a sequence of ids), using the vocabulary.\n",
    "        \"\"\"\n",
    "        if tokens is None:\n",
    "            return None\n",
    "\n",
    "        if isinstance(tokens, str):\n",
    "            return self._convert_token_to_id_with_added_voc(tokens)\n",
    "\n",
    "        ids = []\n",
    "        for token in tokens:\n",
    "            ids.append(self._convert_token_to_id_with_added_voc(token))\n",
    "        return ids\n",
    "\n",
    "    def _convert_token_to_id_with_added_voc(self, token):\n",
    "        if token is None:\n",
    "            return None\n",
    "\n",
    "        if token in self.added_tokens_encoder:\n",
    "            return self.added_tokens_encoder[token]\n",
    "        return self._convert_token_to_id(token)\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        text: str,\n",
    "        text_pair: Optional[str] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        truncation_strategy: str = \"longest_first\",\n",
    "        pad_to_max_length: bool = False,\n",
    "        return_tensors: Optional[str] = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Converts a string in a sequence of ids (integer), using the tokenizer and vocabulary.\n",
    "\n",
    "        Same as doing ``self.convert_tokens_to_ids(self.tokenize(text))``.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str` or :obj:`List[str]`):\n",
    "                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
    "                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
    "                method)\n",
    "            text_pair (:obj:`str` or :obj:`List[str]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
    "                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n",
    "                `convert_tokens_to_ids` method)\n",
    "            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                If set to ``True``, the sequences will be encoded with the special tokens relative\n",
    "                to their model.\n",
    "            max_length (:obj:`int`, `optional`, defaults to :obj:`None`):\n",
    "                If set to a number, will limit the total sequence returned so that it has a maximum length.\n",
    "                If there are overflowing tokens, those will be added to the returned dictionary\n",
    "            stride (:obj:`int`, `optional`, defaults to ``0``):\n",
    "                If set to a number along with max_length, the overflowing tokens returned will contain some tokens\n",
    "                from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
    "            truncation_strategy (:obj:`str`, `optional`, defaults to `longest_first`):\n",
    "                String selected in the following options:\n",
    "\n",
    "                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
    "                  starting from the longest one at each token (when there is a pair of input sequences)\n",
    "                - 'only_first': Only truncate the first sequence\n",
    "                - 'only_second': Only truncate the second sequence\n",
    "                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
    "            pad_to_max_length (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                If set to True, the returned sequences will be padded according to the model's padding side and\n",
    "                padding index, up to their max length. If no max length is specified, the padding is done up to the\n",
    "                model's max length. The tokenizer padding sides are handled by the class attribute `padding_side`\n",
    "                which can be set to the following strings:\n",
    "\n",
    "                - 'left': pads on the left of the sequences\n",
    "                - 'right': pads on the right of the sequences\n",
    "                Defaults to False: no padding.\n",
    "            return_tensors (:obj:`str`, `optional`, defaults to :obj:`None`):\n",
    "                Can be set to 'tf' or 'pt' to return respectively TensorFlow :obj:`tf.constant`\n",
    "                or PyTorch :obj:`torch.Tensor` instead of a list of python integers.\n",
    "            **kwargs: passed to the `self.tokenize()` method\n",
    "        \"\"\"\n",
    "        encoded_inputs = self.encode_plus(\n",
    "            text,\n",
    "            text_pair=text_pair,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            stride=stride,\n",
    "            truncation_strategy=truncation_strategy,\n",
    "            pad_to_max_length=pad_to_max_length,\n",
    "            return_tensors=return_tensors,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        return encoded_inputs[\"input_ids\"]\n",
    "\n",
    "    def encode_plus(\n",
    "        self,\n",
    "        text: str,\n",
    "        text_pair: Optional[str] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        truncation_strategy: str = \"longest_first\",\n",
    "        pad_to_max_length: bool = False,\n",
    "        return_tensors: Optional[str] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing the encoded sequence or sequence pair and additional information:\n",
    "        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n",
    "\n",
    "        Args:\n",
    "            text (:obj:`str` or :obj:`List[str]`):\n",
    "                The first sequence to be encoded. This can be a string, a list of strings (tokenized string using\n",
    "                the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n",
    "                method)\n",
    "            text_pair (:obj:`str` or :obj:`List[str]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second sequence to be encoded. This can be a string, a list of strings (tokenized\n",
    "                string using the `tokenize` method) or a list of integers (tokenized string ids using the\n",
    "                `convert_tokens_to_ids` method)\n",
    "            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                If set to ``True``, the sequences will be encoded with the special tokens relative\n",
    "                to their model.\n",
    "            max_length (:obj:`int`, `optional`, defaults to :obj:`None`):\n",
    "                If set to a number, will limit the total sequence returned so that it has a maximum length.\n",
    "                If there are overflowing tokens, those will be added to the returned dictionary\n",
    "            stride (:obj:`int`, `optional`, defaults to ``0``):\n",
    "                If set to a number along with max_length, the overflowing tokens returned will contain some tokens\n",
    "                from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
    "            truncation_strategy (:obj:`str`, `optional`, defaults to `longest_first`):\n",
    "                String selected in the following options:\n",
    "\n",
    "                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
    "                  starting from the longest one at each token (when there is a pair of input sequences)\n",
    "                - 'only_first': Only truncate the first sequence\n",
    "                - 'only_second': Only truncate the second sequence\n",
    "                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
    "            pad_to_max_length (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                If set to True, the returned sequences will be padded according to the model's padding side and\n",
    "                padding index, up to their max length. If no max length is specified, the padding is done up to the\n",
    "                model's max length. The tokenizer padding sides are handled by the class attribute `padding_side`\n",
    "                which can be set to the following strings:\n",
    "\n",
    "                - 'left': pads on the left of the sequences\n",
    "                - 'right': pads on the right of the sequences\n",
    "                Defaults to False: no padding.\n",
    "            return_tensors (:obj:`str`, `optional`, defaults to :obj:`None`):\n",
    "                Can be set to 'tf' or 'pt' to return respectively TensorFlow :obj:`tf.constant`\n",
    "                or PyTorch :obj:`torch.Tensor` instead of a list of python integers.\n",
    "            return_token_type_ids (:obj:`bool`, `optional`, defaults to :obj:`None`):\n",
    "                Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
    "                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
    "\n",
    "                `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
    "            return_attention_mask (:obj:`bool`, `optional`, defaults to :obj:`none`):\n",
    "                Whether to return the attention mask. If left to the default, will return the attention mask according\n",
    "                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
    "\n",
    "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "            return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True to return overflowing token information (default False).\n",
    "            return_special_tokens_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True to return special tokens mask information (default False).\n",
    "            return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True to return (char_start, char_end) for each token (default False).\n",
    "                If using Python's tokenizer, this method will raise NotImplementedError. This one is only available on\n",
    "                Rust-based tokenizers inheriting from PreTrainedTokenizerFast.\n",
    "            **kwargs: passed to the `self.tokenize()` method\n",
    "\n",
    "        Return:\n",
    "            A Dictionary of shape::\n",
    "\n",
    "                {\n",
    "                    input_ids: list[int],\n",
    "                    token_type_ids: list[int] if return_token_type_ids is True (default)\n",
    "                    attention_mask: list[int] if return_attention_mask is True (default)\n",
    "                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
    "                }\n",
    "\n",
    "            With the fields:\n",
    "\n",
    "            - ``input_ids``: list of token ids to be fed to a model\n",
    "            - ``token_type_ids``: list of token type ids to be fed to a model\n",
    "            - ``attention_mask``: list of indices specifying which tokens should be attended to by the model\n",
    "            - ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n",
    "            - ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n",
    "            - ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n",
    "              tokens and 1 specifying sequence tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        def get_input_ids(text):\n",
    "            if isinstance(text, str):\n",
    "                tokens = self.tokenize(text, add_special_tokens=add_special_tokens, **kwargs)\n",
    "                return self.convert_tokens_to_ids(tokens)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], str):\n",
    "                return self.convert_tokens_to_ids(text)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], int):\n",
    "                return text\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\n",
    "                )\n",
    "\n",
    "        if return_offsets_mapping:\n",
    "            raise NotImplementedError(\n",
    "                \"return_offset_mapping is not available when using Python tokenizers.\"\n",
    "                \"To use this feature, change your tokenizer to one deriving from \"\n",
    "                \"transformers.PreTrainedTokenizerFast.\"\n",
    "                \"More information on available tokenizers at \"\n",
    "                \"https://github.com/huggingface/transformers/pull/2674\"\n",
    "            )\n",
    "\n",
    "        # Throw an error if we can pad because there is no padding token\n",
    "        if pad_to_max_length and self.pad_token_id is None:\n",
    "            raise ValueError(\n",
    "                \"Unable to set proper padding strategy as the tokenizer does not have a padding token. In this case please set the `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via the function add_special_tokens if you want to use a padding strategy\"\n",
    "            )\n",
    "\n",
    "        first_ids = get_input_ids(text)\n",
    "        second_ids = get_input_ids(text_pair) if text_pair is not None else None\n",
    "\n",
    "        return self.prepare_for_model(\n",
    "            first_ids,\n",
    "            pair_ids=second_ids,\n",
    "            max_length=max_length,\n",
    "            pad_to_max_length=pad_to_max_length,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            stride=stride,\n",
    "            truncation_strategy=truncation_strategy,\n",
    "            return_tensors=return_tensors,\n",
    "            return_attention_mask=return_attention_mask,\n",
    "            return_token_type_ids=return_token_type_ids,\n",
    "            return_overflowing_tokens=return_overflowing_tokens,\n",
    "            return_special_tokens_mask=return_special_tokens_mask,\n",
    "        )\n",
    "\n",
    "    def batch_encode_plus(\n",
    "        self,\n",
    "        batch_text_or_text_pairs: Union[str, List[str]],\n",
    "        add_special_tokens: bool = True,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        truncation_strategy: str = \"longest_first\",\n",
    "        pad_to_max_length: bool = False,\n",
    "        return_tensors: Optional[str] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_masks: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_masks: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        return_input_lengths: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Returns a dictionary containing the encoded sequence or sequence pair and additional information:\n",
    "        the mask for sequence classification and the overflowing elements if a ``max_length`` is specified.\n",
    "\n",
    "        Args:\n",
    "            batch_text_or_text_pairs (:obj:`List[str]` or :obj:`List[List[str]]`):\n",
    "                Batch of sequences or pair of sequences to be encoded.\n",
    "                This can be a list of string/string-sequences/int-sequences or a list of pair of\n",
    "                string/string-sequences/int-sequence (see details in encode_plus)\n",
    "            add_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`True`):\n",
    "                If set to ``True``, the sequences will be encoded with the special tokens relative\n",
    "                to their model.\n",
    "            max_length (:obj:`int`, `optional`, defaults to :obj:`None`):\n",
    "                If set to a number, will limit the total sequence returned so that it has a maximum length.\n",
    "                If there are overflowing tokens, those will be added to the returned dictionary\n",
    "            stride (:obj:`int`, `optional`, defaults to ``0``):\n",
    "                If set to a number along with max_length, the overflowing tokens returned will contain some tokens\n",
    "                from the main sequence returned. The value of this argument defines the number of additional tokens.\n",
    "            truncation_strategy (:obj:`str`, `optional`, defaults to `longest_first`):\n",
    "                String selected in the following options:\n",
    "\n",
    "                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
    "                  starting from the longest one at each token (when there is a pair of input sequences)\n",
    "                - 'only_first': Only truncate the first sequence\n",
    "                - 'only_second': Only truncate the second sequence\n",
    "                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
    "            pad_to_max_length (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                If set to True, the returned sequences will be padded according to the model's padding side and\n",
    "                padding index, up to their max length. If no max length is specified, the padding is done up to the\n",
    "                model's max length. The tokenizer padding sides are handled by the class attribute `padding_side`\n",
    "                which can be set to the following strings:\n",
    "\n",
    "                - 'left': pads on the left of the sequences\n",
    "                - 'right': pads on the right of the sequences\n",
    "                Defaults to False: no padding.\n",
    "            return_tensors (:obj:`str`, `optional`, defaults to :obj:`None`):\n",
    "                Can be set to 'tf' or 'pt' to return respectively TensorFlow :obj:`tf.constant`\n",
    "                or PyTorch :obj:`torch.Tensor` instead of a list of python integers.\n",
    "            return_token_type_ids (:obj:`bool`, `optional`, defaults to :obj:`None`):\n",
    "                Whether to return token type IDs. If left to the default, will return the token type IDs according\n",
    "                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
    "\n",
    "                `What are token type IDs? <../glossary.html#token-type-ids>`_\n",
    "            return_attention_masks (:obj:`bool`, `optional`, defaults to :obj:`none`):\n",
    "                Whether to return the attention mask. If left to the default, will return the attention mask according\n",
    "                to the specific tokenizer's default, defined by the :obj:`return_outputs` attribute.\n",
    "\n",
    "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
    "            return_overflowing_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True to return overflowing token information (default False).\n",
    "            return_special_tokens_masks (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True to return special tokens mask information (default False).\n",
    "            return_offsets_mapping (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True to return (char_start, char_end) for each token (default False).\n",
    "                If using Python's tokenizer, this method will raise NotImplementedError. This one is only available on\n",
    "                Rust-based tokenizers inheriting from PreTrainedTokenizerFast.\n",
    "            return_input_lengths (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                If set the resulting dictionary will include the length of each sample\n",
    "            **kwargs: passed to the `self.tokenize()` method\n",
    "\n",
    "        Return:\n",
    "            A Dictionary of shape::\n",
    "\n",
    "                {\n",
    "                    input_ids: list[List[int]],\n",
    "                    token_type_ids: list[List[int]] if return_token_type_ids is True (default)\n",
    "                    attention_mask: list[List[int]] if return_attention_mask is True (default)\n",
    "                    overflowing_tokens: list[List[int]] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "                    num_truncated_tokens: List[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "                    special_tokens_mask: list[List[int]] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
    "                }\n",
    "\n",
    "            With the fields:\n",
    "\n",
    "            - ``input_ids``: list of token ids to be fed to a model\n",
    "            - ``token_type_ids``: list of token type ids to be fed to a model\n",
    "            - ``attention_mask``: list of indices specifying which tokens should be attended to by the model\n",
    "            - ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n",
    "            - ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n",
    "            - ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n",
    "              tokens and 1 specifying sequence tokens.\n",
    "        \"\"\"\n",
    "\n",
    "        def get_input_ids(text):\n",
    "            if isinstance(text, str):\n",
    "                tokens = self.tokenize(text, add_special_tokens=add_special_tokens, **kwargs)\n",
    "                return self.convert_tokens_to_ids(tokens)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], str):\n",
    "                return self.convert_tokens_to_ids(text)\n",
    "            elif isinstance(text, (list, tuple)) and len(text) > 0 and isinstance(text[0], int):\n",
    "                return text\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Input is not valid. Should be a string, a list/tuple of strings or a list/tuple of integers.\"\n",
    "                )\n",
    "\n",
    "        # Throw an error if we can pad because there is no padding token\n",
    "        if pad_to_max_length and self.pad_token_id is None:\n",
    "            raise ValueError(\n",
    "                \"Unable to set proper padding strategy as the tokenizer does not have a padding token. In this case please set the `pad_token` `(tokenizer.pad_token = tokenizer.eos_token e.g.)` or add a new pad token via the function add_special_tokens if you want to use a padding strategy\"\n",
    "            )\n",
    "\n",
    "        if return_offsets_mapping:\n",
    "            raise NotImplementedError(\n",
    "                \"return_offset_mapping is not available when using Python tokenizers.\"\n",
    "                \"To use this feature, change your tokenizer to one deriving from \"\n",
    "                \"transformers.PreTrainedTokenizerFast.\"\n",
    "                \"More information on available tokenizers at \"\n",
    "                \"https://github.com/huggingface/transformers/pull/2674\"\n",
    "            )\n",
    "\n",
    "        input_ids = []\n",
    "        for ids_or_pair_ids in batch_text_or_text_pairs:\n",
    "            if isinstance(ids_or_pair_ids, (list, tuple)) and len(ids_or_pair_ids) == 2:\n",
    "                ids, pair_ids = ids_or_pair_ids\n",
    "            else:\n",
    "                ids, pair_ids = ids_or_pair_ids, None\n",
    "\n",
    "            first_ids = get_input_ids(ids)\n",
    "            second_ids = get_input_ids(pair_ids) if pair_ids is not None else None\n",
    "            input_ids.append((first_ids, second_ids))\n",
    "\n",
    "        if max_length is None and pad_to_max_length:\n",
    "\n",
    "            def total_sequence_length(input_pairs):\n",
    "                first_ids, second_ids = input_pairs\n",
    "                return len(first_ids) + (\n",
    "                    self.num_added_tokens()\n",
    "                    if second_ids is None\n",
    "                    else (len(second_ids) + self.num_added_tokens(pair=True))\n",
    "                )\n",
    "\n",
    "            max_length = max([total_sequence_length(ids) for ids in input_ids])\n",
    "\n",
    "        batch_outputs = {}\n",
    "        for first_ids, second_ids in input_ids:\n",
    "            # Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by\n",
    "            # the model. It adds special tokens, truncates sequences if overflowing while taking into account\n",
    "            # the special tokens and manages a window stride for overflowing tokens\n",
    "            outputs = self.prepare_for_model(\n",
    "                first_ids,\n",
    "                pair_ids=second_ids,\n",
    "                max_length=max_length,\n",
    "                pad_to_max_length=pad_to_max_length,\n",
    "                add_special_tokens=add_special_tokens,\n",
    "                stride=stride,\n",
    "                truncation_strategy=truncation_strategy,\n",
    "                return_attention_mask=return_attention_masks,\n",
    "                return_token_type_ids=return_token_type_ids,\n",
    "                return_overflowing_tokens=return_overflowing_tokens,\n",
    "                return_special_tokens_mask=return_special_tokens_masks,\n",
    "            )\n",
    "\n",
    "            # Append the non-padded length to the output\n",
    "            if return_input_lengths:\n",
    "                outputs[\"input_len\"] = len(outputs[\"input_ids\"])\n",
    "\n",
    "            for key, value in outputs.items():\n",
    "                if key not in batch_outputs:\n",
    "                    batch_outputs[key] = []\n",
    "                batch_outputs[key].append(value)\n",
    "\n",
    "        if return_tensors is not None:\n",
    "\n",
    "            # Do the tensor conversion in batch\n",
    "            for key, value in batch_outputs.items():\n",
    "                if return_tensors == \"tf\" and is_tf_available():\n",
    "                    try:\n",
    "                        batch_outputs[key] = tf.constant(value)\n",
    "                    except ValueError:\n",
    "                        if None in [item for sequence in value for item in sequence]:\n",
    "                            raise ValueError(self.NO_PAD_TOKEN_FOR_BATCH_MSG)\n",
    "                        else:\n",
    "                            raise ValueError(self.UNEVEN_SEQUENCES_FOR_BATCH_MSG)\n",
    "                elif return_tensors == \"pt\" and is_torch_available():\n",
    "                    try:\n",
    "                        batch_outputs[key] = torch.tensor(value)\n",
    "                    except ValueError:\n",
    "                        raise ValueError(self.UNEVEN_SEQUENCES_FOR_BATCH_MSG)\n",
    "                    except RuntimeError:\n",
    "                        if None in [item for sequence in value for item in sequence]:\n",
    "                            raise ValueError(self.NO_PAD_TOKEN_FOR_BATCH_MSG)\n",
    "                        else:\n",
    "                            raise\n",
    "                elif return_tensors is not None:\n",
    "                    logger.warning(\n",
    "                        \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                            return_tensors\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "        return batch_outputs\n",
    "\n",
    "    def prepare_for_model(\n",
    "        self,\n",
    "        ids: List[int],\n",
    "        pair_ids: Optional[List[int]] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        stride: int = 0,\n",
    "        truncation_strategy: str = \"longest_first\",\n",
    "        pad_to_max_length: bool = False,\n",
    "        return_tensors: Optional[str] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Prepares a sequence of input id, or a pair of sequences of inputs ids so that it can be used by the model.\n",
    "        It adds special tokens, truncates\n",
    "        sequences if overflowing while taking into account the special tokens and manages a window stride for\n",
    "        overflowing tokens\n",
    "\n",
    "        Args:\n",
    "            ids: list of tokenized input ids. Can be obtained from a string by chaining the\n",
    "                `tokenize` and `convert_tokens_to_ids` methods.\n",
    "            pair_ids: Optional second list of input ids. Can be obtained from a string by chaining the\n",
    "                `tokenize` and `convert_tokens_to_ids` methods.\n",
    "            max_length: maximum length of the returned list. Will truncate by taking into account the special tokens.\n",
    "            add_special_tokens: if set to ``True``, the sequences will be encoded with the special tokens relative\n",
    "                to their model.\n",
    "            stride: window stride for overflowing tokens. Can be useful for edge effect removal when using sequential\n",
    "                list of inputs.\n",
    "            truncation_strategy: string selected in the following options:\n",
    "                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
    "                    starting from the longest one at each token (when there is a pair of input sequences)\n",
    "                - 'only_first': Only truncate the first sequence\n",
    "                - 'only_second': Only truncate the second sequence\n",
    "                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
    "            pad_to_max_length: if set to True, the returned sequences will be padded according to the model's padding side and\n",
    "                padding index, up to their max length. If no max length is specified, the padding is done up to the model's max length.\n",
    "                The tokenizer padding sides are handled by the following strings:\n",
    "                - 'left': pads on the left of the sequences\n",
    "                - 'right': pads on the right of the sequences\n",
    "                Defaults to False: no padding.\n",
    "            return_tensors: (optional) can be set to 'tf' or 'pt' to return respectively TensorFlow tf.constant\n",
    "                or PyTorch torch.Tensor instead of a list of python integers.\n",
    "            return_token_type_ids: (optional) Set to False to avoid returning token_type_ids (default True).\n",
    "            return_attention_mask: (optional) Set to False to avoid returning attention mask (default True)\n",
    "            return_overflowing_tokens: (optional) Set to True to return overflowing token information (default False).\n",
    "            return_special_tokens_mask: (optional) Set to True to return special tokens mask information (default False).\n",
    "\n",
    "        Return:\n",
    "            A Dictionary of shape::\n",
    "\n",
    "                {\n",
    "                    input_ids: list[int],\n",
    "                    token_type_ids: list[int] if return_token_type_ids is True (default)\n",
    "                    overflowing_tokens: list[int] if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "                    num_truncated_tokens: int if a ``max_length`` is specified and return_overflowing_tokens is True\n",
    "                    special_tokens_mask: list[int] if ``add_special_tokens`` if set to ``True`` and return_special_tokens_mask is True\n",
    "                }\n",
    "\n",
    "            With the fields:\n",
    "                ``input_ids``: list of token ids to be fed to a model\n",
    "                ``token_type_ids``: list of token type ids to be fed to a model\n",
    "\n",
    "                ``overflowing_tokens``: list of overflowing tokens if a max length is specified.\n",
    "                ``num_truncated_tokens``: number of overflowing tokens a ``max_length`` is specified\n",
    "                ``special_tokens_mask``: if adding special tokens, this is a list of [0, 1], with 0 specifying special added\n",
    "                tokens and 1 specifying sequence tokens.\n",
    "        \"\"\"\n",
    "        pair = bool(pair_ids is not None)\n",
    "        len_ids = len(ids)\n",
    "        len_pair_ids = len(pair_ids) if pair else 0\n",
    "\n",
    "        if return_token_type_ids is None:\n",
    "            return_token_type_ids = \"token_type_ids\" in self.model_input_names\n",
    "        if return_attention_mask is None:\n",
    "            return_attention_mask = \"attention_mask\" in self.model_input_names\n",
    "\n",
    "        encoded_inputs = {}\n",
    "\n",
    "        # Handle max sequence length\n",
    "        total_len = len_ids + len_pair_ids + (self.num_added_tokens(pair=pair) if add_special_tokens else 0)\n",
    "        if max_length and total_len > max_length:\n",
    "            ids, pair_ids, overflowing_tokens = self.truncate_sequences(\n",
    "                ids,\n",
    "                pair_ids=pair_ids,\n",
    "                num_tokens_to_remove=total_len - max_length,\n",
    "                truncation_strategy=truncation_strategy,\n",
    "                stride=stride,\n",
    "            )\n",
    "            if return_overflowing_tokens:\n",
    "                encoded_inputs[\"overflowing_tokens\"] = overflowing_tokens\n",
    "                encoded_inputs[\"num_truncated_tokens\"] = total_len - max_length\n",
    "\n",
    "        # Handle special_tokens\n",
    "        if add_special_tokens:\n",
    "            sequence = self.build_inputs_with_special_tokens(ids, pair_ids)\n",
    "            token_type_ids = self.create_token_type_ids_from_sequences(ids, pair_ids)\n",
    "        else:\n",
    "            sequence = ids + pair_ids if pair else ids\n",
    "            token_type_ids = [0] * len(ids) + ([1] * len(pair_ids) if pair else [])\n",
    "\n",
    "        if return_special_tokens_mask:\n",
    "            if add_special_tokens:\n",
    "                encoded_inputs[\"special_tokens_mask\"] = self.get_special_tokens_mask(ids, pair_ids)\n",
    "            else:\n",
    "                encoded_inputs[\"special_tokens_mask\"] = [0] * len(sequence)\n",
    "\n",
    "        encoded_inputs[\"input_ids\"] = sequence\n",
    "        if return_token_type_ids:\n",
    "            encoded_inputs[\"token_type_ids\"] = token_type_ids\n",
    "\n",
    "        if max_length and len(encoded_inputs[\"input_ids\"]) > max_length:\n",
    "            encoded_inputs[\"input_ids\"] = encoded_inputs[\"input_ids\"][:max_length]\n",
    "            if return_token_type_ids:\n",
    "                encoded_inputs[\"token_type_ids\"] = encoded_inputs[\"token_type_ids\"][:max_length]\n",
    "            if return_special_tokens_mask:\n",
    "                encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"][:max_length]\n",
    "\n",
    "        if max_length is None and len(encoded_inputs[\"input_ids\"]) > self.max_len:\n",
    "            logger.warning(\n",
    "                \"Token indices sequence length is longer than the specified maximum sequence length \"\n",
    "                \"for this model ({} > {}). Running this sequence through the model will result in \"\n",
    "                \"indexing errors\".format(len(ids), self.max_len)\n",
    "            )\n",
    "\n",
    "        needs_to_be_padded = pad_to_max_length and (\n",
    "            max_length\n",
    "            and len(encoded_inputs[\"input_ids\"]) < max_length\n",
    "            or max_length is None\n",
    "            and len(encoded_inputs[\"input_ids\"]) < self.max_len\n",
    "            and self.max_len <= 10000\n",
    "        )\n",
    "\n",
    "        if pad_to_max_length and max_length is None and self.max_len > 10000:\n",
    "            logger.warning(\n",
    "                \"Sequence can't be padded as no maximum length is specified and the model maximum length is too high.\"\n",
    "            )\n",
    "\n",
    "        if needs_to_be_padded:\n",
    "            difference = (max_length if max_length is not None else self.max_len) - len(encoded_inputs[\"input_ids\"])\n",
    "\n",
    "            if self.padding_side == \"right\":\n",
    "                if return_attention_mask:\n",
    "                    encoded_inputs[\"attention_mask\"] = [1] * len(encoded_inputs[\"input_ids\"]) + [0] * difference\n",
    "                if return_token_type_ids:\n",
    "                    encoded_inputs[\"token_type_ids\"] = (\n",
    "                        encoded_inputs[\"token_type_ids\"] + [self.pad_token_type_id] * difference\n",
    "                    )\n",
    "                if return_special_tokens_mask:\n",
    "                    encoded_inputs[\"special_tokens_mask\"] = encoded_inputs[\"special_tokens_mask\"] + [1] * difference\n",
    "                encoded_inputs[\"input_ids\"] = encoded_inputs[\"input_ids\"] + [self.pad_token_id] * difference\n",
    "            elif self.padding_side == \"left\":\n",
    "                if return_attention_mask:\n",
    "                    encoded_inputs[\"attention_mask\"] = [0] * difference + [1] * len(encoded_inputs[\"input_ids\"])\n",
    "                if return_token_type_ids:\n",
    "                    encoded_inputs[\"token_type_ids\"] = [self.pad_token_type_id] * difference + encoded_inputs[\n",
    "                        \"token_type_ids\"\n",
    "                    ]\n",
    "                if return_special_tokens_mask:\n",
    "                    encoded_inputs[\"special_tokens_mask\"] = [1] * difference + encoded_inputs[\"special_tokens_mask\"]\n",
    "                encoded_inputs[\"input_ids\"] = [self.pad_token_id] * difference + encoded_inputs[\"input_ids\"]\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Invalid padding strategy:\" + str(self.padding_side))\n",
    "\n",
    "        elif return_attention_mask:\n",
    "            encoded_inputs[\"attention_mask\"] = [1] * len(encoded_inputs[\"input_ids\"])\n",
    "\n",
    "        # Prepare inputs as tensors if asked\n",
    "        if return_tensors == \"tf\" and is_tf_available():\n",
    "            encoded_inputs[\"input_ids\"] = tf.constant([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "            if \"token_type_ids\" in encoded_inputs:\n",
    "                encoded_inputs[\"token_type_ids\"] = tf.constant([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "            if \"attention_mask\" in encoded_inputs:\n",
    "                encoded_inputs[\"attention_mask\"] = tf.constant([encoded_inputs[\"attention_mask\"]])\n",
    "\n",
    "        elif return_tensors == \"pt\" and is_torch_available():\n",
    "            encoded_inputs[\"input_ids\"] = torch.tensor([encoded_inputs[\"input_ids\"]])\n",
    "\n",
    "            if \"token_type_ids\" in encoded_inputs:\n",
    "                encoded_inputs[\"token_type_ids\"] = torch.tensor([encoded_inputs[\"token_type_ids\"]])\n",
    "\n",
    "            if \"attention_mask\" in encoded_inputs:\n",
    "                encoded_inputs[\"attention_mask\"] = torch.tensor([encoded_inputs[\"attention_mask\"]])\n",
    "        elif return_tensors is not None:\n",
    "            logger.warning(\n",
    "                \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                    return_tensors\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return encoded_inputs\n",
    "\n",
    "    def prepare_for_tokenization(self, text, **kwargs):\n",
    "        \"\"\" Performs any necessary transformations before tokenization \"\"\"\n",
    "        return text\n",
    "\n",
    "    def truncate_sequences(\n",
    "        self, ids, pair_ids=None, num_tokens_to_remove=0, truncation_strategy=\"longest_first\", stride=0\n",
    "    ):\n",
    "        \"\"\"Truncates a sequence pair in place to the maximum length.\n",
    "            truncation_strategy: string selected in the following options:\n",
    "                - 'longest_first' (default) Iteratively reduce the inputs sequence until the input is under max_length\n",
    "                    starting from the longest one at each token (when there is a pair of input sequences).\n",
    "                    Overflowing tokens only contains overflow from the first sequence.\n",
    "                - 'only_first': Only truncate the first sequence. raise an error if the first sequence is shorter or equal to than num_tokens_to_remove.\n",
    "                - 'only_second': Only truncate the second sequence\n",
    "                - 'do_not_truncate': Does not truncate (raise an error if the input sequence is longer than max_length)\n",
    "        \"\"\"\n",
    "        if num_tokens_to_remove <= 0:\n",
    "            return ids, pair_ids, []\n",
    "\n",
    "        if truncation_strategy == \"longest_first\":\n",
    "            overflowing_tokens = []\n",
    "            for _ in range(num_tokens_to_remove):\n",
    "                if pair_ids is None or len(ids) > len(pair_ids):\n",
    "                    overflowing_tokens = [ids[-1]] + overflowing_tokens\n",
    "                    ids = ids[:-1]\n",
    "                else:\n",
    "                    pair_ids = pair_ids[:-1]\n",
    "            window_len = min(len(ids), stride)\n",
    "            if window_len > 0:\n",
    "                overflowing_tokens = ids[-window_len:] + overflowing_tokens\n",
    "        elif truncation_strategy == \"only_first\":\n",
    "            assert len(ids) > num_tokens_to_remove\n",
    "            window_len = min(len(ids), stride + num_tokens_to_remove)\n",
    "            overflowing_tokens = ids[-window_len:]\n",
    "            ids = ids[:-num_tokens_to_remove]\n",
    "        elif truncation_strategy == \"only_second\":\n",
    "            assert pair_ids is not None and len(pair_ids) > num_tokens_to_remove\n",
    "            window_len = min(len(pair_ids), stride + num_tokens_to_remove)\n",
    "            overflowing_tokens = pair_ids[-window_len:]\n",
    "            pair_ids = pair_ids[:-num_tokens_to_remove]\n",
    "        elif truncation_strategy == \"do_not_truncate\":\n",
    "            raise ValueError(\"Input sequence are too long for max_length. Please select a truncation strategy.\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Truncation_strategy should be selected in ['longest_first', 'only_first', 'only_second', 'do_not_truncate']\"\n",
    "            )\n",
    "        return (ids, pair_ids, overflowing_tokens)\n",
    "\n",
    "    def create_token_type_ids_from_sequences(self, token_ids_0, token_ids_1=None):\n",
    "        if token_ids_1 is None:\n",
    "            return len(token_ids_0) * [0]\n",
    "        return [0] * len(token_ids_0) + [1] * len(token_ids_1)\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A RoBERTa sequence has the following format:\n",
    "            single sequence: <s> X </s>\n",
    "            pair of sequences: <s> A </s></s> B </s>\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return token_ids_0\n",
    "        return token_ids_0 + token_ids_1\n",
    "\n",
    "    def get_special_tokens_mask(self, token_ids_0, token_ids_1=None, already_has_special_tokens=False):\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0: list of ids (must not contain special tokens)\n",
    "            token_ids_1: Optional list of ids (must not contain special tokens), necessary when fetching sequence ids\n",
    "                for sequence pairs\n",
    "            already_has_special_tokens: (default False) Set to True if the token list is already formated with\n",
    "                special tokens for the model\n",
    "\n",
    "        Returns:\n",
    "            A list of integers in the range [0, 1]: 1 for a special token, 0 for a sequence token.\n",
    "        \"\"\"\n",
    "        return [0] * ((len(token_ids_1) if token_ids_1 else 0) + len(token_ids_0))\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids, skip_special_tokens=False):\n",
    "        \"\"\" Converts a single index or a sequence of indices (integers) in a token \"\n",
    "            (resp.) a sequence of tokens (str), using the vocabulary and added tokens.\n",
    "\n",
    "            Args:\n",
    "                skip_special_tokens: Don't decode special tokens (self.all_special_tokens). Default: False\n",
    "        \"\"\"\n",
    "        if isinstance(ids, int):\n",
    "            if ids in self.added_tokens_decoder:\n",
    "                return self.added_tokens_decoder[ids]\n",
    "            else:\n",
    "                return self._convert_id_to_token(ids)\n",
    "        tokens = []\n",
    "        for index in ids:\n",
    "            index = int(index)\n",
    "            if skip_special_tokens and index in self.all_special_ids:\n",
    "                continue\n",
    "            if index in self.added_tokens_decoder:\n",
    "                tokens.append(self.added_tokens_decoder[index])\n",
    "            else:\n",
    "                tokens.append(self._convert_id_to_token(index))\n",
    "        return tokens\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\" Converts a sequence of tokens (string) in a single string.\n",
    "            The most simple way to do it is ' '.join(self.convert_ids_to_tokens(token_ids))\n",
    "            but we often want to remove sub-word tokenization artifacts at the same time.\n",
    "        \"\"\"\n",
    "        return \" \".join(self.convert_ids_to_tokens(tokens))\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n",
    "        \"\"\"\n",
    "        Converts a sequence of ids (integer) in a string, using the tokenizer and vocabulary\n",
    "        with options to remove special tokens and clean up tokenization spaces.\n",
    "        Similar to doing ``self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))``.\n",
    "\n",
    "        Args:\n",
    "            token_ids: list of tokenized input ids. Can be obtained using the `encode` or `encode_plus` methods.\n",
    "            skip_special_tokens: if set to True, will replace special tokens.\n",
    "            clean_up_tokenization_spaces: if set to True, will clean up the tokenization spaces.\n",
    "        \"\"\"\n",
    "        filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=skip_special_tokens)\n",
    "\n",
    "        # To avoid mixing byte-level and unicode for byte-level BPT\n",
    "        # we need to build string separatly for added tokens and byte-level tokens\n",
    "        # cf. https://github.com/huggingface/transformers/issues/1133\n",
    "        sub_texts = []\n",
    "        current_sub_text = []\n",
    "        for token in filtered_tokens:\n",
    "            if skip_special_tokens and token in self.all_special_ids:\n",
    "                continue\n",
    "            if token in self.added_tokens_encoder:\n",
    "                if current_sub_text:\n",
    "                    sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n",
    "                    current_sub_text = []\n",
    "                sub_texts.append(token)\n",
    "            else:\n",
    "                current_sub_text.append(token)\n",
    "        if current_sub_text:\n",
    "            sub_texts.append(self.convert_tokens_to_string(current_sub_text))\n",
    "        text = \" \".join(sub_texts)\n",
    "\n",
    "        if clean_up_tokenization_spaces:\n",
    "            clean_text = self.clean_up_tokenization(text)\n",
    "            return clean_text\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    @property\n",
    "    def special_tokens_map(self):\n",
    "        \"\"\" A dictionary mapping special token class attribute (cls_token, unk_token...) to their\n",
    "            values ('<unk>', '<cls>'...)\n",
    "        \"\"\"\n",
    "        set_attr = {}\n",
    "        for attr in self.SPECIAL_TOKENS_ATTRIBUTES:\n",
    "            attr_value = getattr(self, \"_\" + attr)\n",
    "            if attr_value:\n",
    "                set_attr[attr] = attr_value\n",
    "        return set_attr\n",
    "\n",
    "    @property\n",
    "    def all_special_tokens(self):\n",
    "        \"\"\" List all the special tokens ('<unk>', '<cls>'...) mapped to class attributes\n",
    "            (cls_token, unk_token...).\n",
    "        \"\"\"\n",
    "        all_toks = []\n",
    "        set_attr = self.special_tokens_map\n",
    "        for attr_value in set_attr.values():\n",
    "            all_toks = all_toks + (list(attr_value) if isinstance(attr_value, (list, tuple)) else [attr_value])\n",
    "        all_toks = list(set(all_toks))\n",
    "        return all_toks\n",
    "\n",
    "    @property\n",
    "    def all_special_ids(self):\n",
    "        \"\"\" List the vocabulary indices of the special tokens ('<unk>', '<cls>'...) mapped to\n",
    "            class attributes (cls_token, unk_token...).\n",
    "        \"\"\"\n",
    "        all_toks = self.all_special_tokens\n",
    "        all_ids = self.convert_tokens_to_ids(all_toks)\n",
    "        return all_ids\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_up_tokenization(out_string):\n",
    "        \"\"\" Clean up a list of simple English tokenization artifacts like spaces before punctuations and abreviated forms.\n",
    "        \"\"\"\n",
    "        out_string = (\n",
    "            out_string.replace(\" .\", \".\")\n",
    "            .replace(\" ?\", \"?\")\n",
    "            .replace(\" !\", \"!\")\n",
    "            .replace(\" ,\", \",\")\n",
    "            .replace(\" ' \", \"'\")\n",
    "            .replace(\" n't\", \"n't\")\n",
    "            .replace(\" 'm\", \"'m\")\n",
    "            .replace(\" do not\", \" don't\")\n",
    "            .replace(\" 's\", \"'s\")\n",
    "            .replace(\" 've\", \"'ve\")\n",
    "            .replace(\" 're\", \"'re\")\n",
    "        )\n",
    "        return out_string\n",
    "\n",
    "\n",
    "class PreTrainedTokenizerFast(PreTrainedTokenizer):\n",
    "\n",
    "    model_input_names = [\"token_type_ids\", \"attention_mask\"]\n",
    "\n",
    "    def __init__(self, tokenizer: BaseTokenizer, **kwargs):\n",
    "        if tokenizer is None:\n",
    "            raise ValueError(\"Provided tokenizer cannot be None\")\n",
    "        self._tokenizer = tokenizer\n",
    "\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_len_single_sentence = self.max_len - self.num_added_tokens(False)  # take into account special tokens\n",
    "        self.max_len_sentences_pair = self.max_len - self.num_added_tokens(True)  # take into account special tokens\n",
    "\n",
    "    @property\n",
    "    def tokenizer(self):\n",
    "        return self._tokenizer\n",
    "\n",
    "    @property\n",
    "    def decoder(self):\n",
    "        return self._tokenizer._tokenizer.decoder\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return self._tokenizer.get_vocab_size(with_added_tokens=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._tokenizer.get_vocab_size(with_added_tokens=True)\n",
    "\n",
    "    @PreTrainedTokenizer.bos_token.setter\n",
    "    def bos_token(self, value):\n",
    "        self._bos_token = value\n",
    "        self._update_special_tokens()\n",
    "\n",
    "    @PreTrainedTokenizer.eos_token.setter\n",
    "    def eos_token(self, value):\n",
    "        self._eos_token = value\n",
    "        self._update_special_tokens()\n",
    "\n",
    "    @PreTrainedTokenizer.unk_token.setter\n",
    "    def unk_token(self, value):\n",
    "        self._unk_token = value\n",
    "        self._update_special_tokens()\n",
    "\n",
    "    @PreTrainedTokenizer.sep_token.setter\n",
    "    def sep_token(self, value):\n",
    "        self._sep_token = value\n",
    "        self._update_special_tokens()\n",
    "\n",
    "    @PreTrainedTokenizer.pad_token.setter\n",
    "    def pad_token(self, value):\n",
    "        self._pad_token = value\n",
    "        self._update_special_tokens()\n",
    "\n",
    "    @PreTrainedTokenizer.cls_token.setter\n",
    "    def cls_token(self, value):\n",
    "        self._cls_token = value\n",
    "        self._update_special_tokens()\n",
    "\n",
    "    @PreTrainedTokenizer.mask_token.setter\n",
    "    def mask_token(self, value):\n",
    "        self._mask_token = value\n",
    "        self._update_special_tokens()\n",
    "\n",
    "    @PreTrainedTokenizer.additional_special_tokens.setter\n",
    "    def additional_special_tokens(self, value):\n",
    "        self._additional_special_tokens = value\n",
    "        self._update_special_tokens()\n",
    "\n",
    "    def _update_special_tokens(self):\n",
    "        if self._tokenizer is not None:\n",
    "            self._tokenizer.add_special_tokens(self.all_special_tokens)\n",
    "\n",
    "    def _convert_encoding(\n",
    "        self,\n",
    "        encoding,\n",
    "        return_tensors=None,\n",
    "        return_token_type_ids=None,\n",
    "        return_attention_mask=None,\n",
    "        return_overflowing_tokens=False,\n",
    "        return_special_tokens_mask=False,\n",
    "        return_offsets_mapping=False,\n",
    "    ):\n",
    "        if return_token_type_ids is None:\n",
    "            return_token_type_ids = \"token_type_ids\" in self.model_input_names\n",
    "        if return_attention_mask is None:\n",
    "            return_attention_mask = \"attention_mask\" in self.model_input_names\n",
    "\n",
    "        if return_overflowing_tokens and encoding.overflowing is not None:\n",
    "            encodings = [encoding] + encoding.overflowing\n",
    "        else:\n",
    "            encodings = [encoding]\n",
    "\n",
    "        encoding_dict = defaultdict(list)\n",
    "        for e in encodings:\n",
    "            encoding_dict[\"input_ids\"].append(e.ids)\n",
    "\n",
    "            if return_token_type_ids:\n",
    "                encoding_dict[\"token_type_ids\"].append(e.type_ids)\n",
    "            if return_attention_mask:\n",
    "                encoding_dict[\"attention_mask\"].append(e.attention_mask)\n",
    "            if return_special_tokens_mask:\n",
    "                encoding_dict[\"special_tokens_mask\"].append(e.special_tokens_mask)\n",
    "            if return_offsets_mapping:\n",
    "                encoding_dict[\"offset_mapping\"].append([e.original_str.offsets(o) for o in e.offsets])\n",
    "\n",
    "        # Prepare inputs as tensors if asked\n",
    "        if return_tensors == \"tf\" and is_tf_available():\n",
    "            encoding_dict[\"input_ids\"] = tf.constant(encoding_dict[\"input_ids\"])\n",
    "            if \"token_type_ids\" in encoding_dict:\n",
    "                encoding_dict[\"token_type_ids\"] = tf.constant(encoding_dict[\"token_type_ids\"])\n",
    "\n",
    "            if \"attention_mask\" in encoding_dict:\n",
    "                encoding_dict[\"attention_mask\"] = tf.constant(encoding_dict[\"attention_mask\"])\n",
    "\n",
    "        elif return_tensors == \"pt\" and is_torch_available():\n",
    "            encoding_dict[\"input_ids\"] = torch.tensor(encoding_dict[\"input_ids\"])\n",
    "            if \"token_type_ids\" in encoding_dict:\n",
    "                encoding_dict[\"token_type_ids\"] = torch.tensor(encoding_dict[\"token_type_ids\"])\n",
    "\n",
    "            if \"attention_mask\" in encoding_dict:\n",
    "                encoding_dict[\"attention_mask\"] = torch.tensor(encoding_dict[\"attention_mask\"])\n",
    "        elif return_tensors is not None:\n",
    "            logger.warning(\n",
    "                \"Unable to convert output to tensors format {}, PyTorch or TensorFlow is not available.\".format(\n",
    "                    return_tensors\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return encoding_dict\n",
    "\n",
    "    def _convert_token_to_id_with_added_voc(self, token):\n",
    "        id = self._tokenizer.token_to_id(token)\n",
    "        if id is None:\n",
    "            return self.unk_token_id\n",
    "        return id\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self._tokenizer.id_to_token(int(index))\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return self._tokenizer.decode(tokens)\n",
    "\n",
    "    def add_tokens(self, new_tokens):\n",
    "        if isinstance(new_tokens, str):\n",
    "            new_tokens = [new_tokens]\n",
    "        return self._tokenizer.add_tokens(new_tokens)\n",
    "\n",
    "    def add_special_tokens(self, special_tokens_dict):\n",
    "        added = super().add_special_tokens(special_tokens_dict)\n",
    "        self._update_special_tokens()\n",
    "        return added\n",
    "\n",
    "    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=None):\n",
    "        if token_ids_1 is None:\n",
    "            return token_ids_0\n",
    "        else:\n",
    "            return token_ids_0 + token_ids_1\n",
    "\n",
    "    def num_added_tokens(self, pair=False):\n",
    "        return self.tokenizer.num_special_tokens_to_add(pair)\n",
    "\n",
    "    def tokenize(self, text, **kwargs):\n",
    "        return self.tokenizer.encode(text).tokens\n",
    "\n",
    "    def batch_encode_plus(\n",
    "        self,\n",
    "        batch_text_or_text_pairs: Optional[Union[List[str], List[Tuple[str]]]] = None,\n",
    "        add_special_tokens: bool = True,\n",
    "        max_length: Optional[int] = None,\n",
    "        stride: int = 0,\n",
    "        truncation_strategy: str = \"longest_first\",\n",
    "        pad_to_max_length: bool = False,\n",
    "        return_tensors: Optional[str] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        if not add_special_tokens:\n",
    "            logger.warning(\n",
    "                \"Fast tokenizers add special tokens by default. To remove special tokens, please specify\"\n",
    "                \"`add_special_tokens=False` during the initialisation rather than when calling `encode`,\"\n",
    "                \"`encode_plus` or `batch_encode_plus`.\"\n",
    "            )\n",
    "\n",
    "        # Needed if we have to return a tensor\n",
    "        pad_to_max_length = pad_to_max_length or (return_tensors is not None)\n",
    "\n",
    "        # Throw an error if we can pad because there is no padding token\n",
    "        if pad_to_max_length and self.pad_token_id is None:\n",
    "            raise ValueError(\"Unable to set proper padding strategy as the tokenizer does not have a padding token\")\n",
    "\n",
    "        # Set the truncation and padding strategy and restore the initial configuration\n",
    "        with truncate_and_pad(\n",
    "            tokenizer=self._tokenizer,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            strategy=truncation_strategy,\n",
    "            pad_to_max_length=pad_to_max_length,\n",
    "            padding_side=self.padding_side,\n",
    "            pad_token_id=self.pad_token_id,\n",
    "            pad_token_type_id=self.pad_token_type_id,\n",
    "            pad_token=self._pad_token,\n",
    "        ):\n",
    "\n",
    "            if not isinstance(batch_text_or_text_pairs, list):\n",
    "                raise TypeError(\n",
    "                    \"batch_text_or_text_pairs has to be a list (got {})\".format(type(batch_text_or_text_pairs))\n",
    "                )\n",
    "\n",
    "            # Avoid thread overhead if only one example.\n",
    "            if len(batch_text_or_text_pairs) == 1:\n",
    "                if isinstance(batch_text_or_text_pairs[0], (tuple, list)):\n",
    "                    tokens = self._tokenizer.encode(*batch_text_or_text_pairs[0])\n",
    "                else:\n",
    "                    tokens = self._tokenizer.encode(batch_text_or_text_pairs[0])\n",
    "                tokens = [tokens]\n",
    "            else:\n",
    "                tokens = self._tokenizer.encode_batch(batch_text_or_text_pairs)\n",
    "\n",
    "        # Convert encoding to dict\n",
    "        tokens = [\n",
    "            self._convert_encoding(\n",
    "                encoding=encoding,\n",
    "                return_tensors=return_tensors,\n",
    "                return_token_type_ids=return_token_type_ids,\n",
    "                return_attention_mask=return_attention_mask,\n",
    "                return_overflowing_tokens=return_overflowing_tokens,\n",
    "                return_special_tokens_mask=return_special_tokens_mask,\n",
    "                return_offsets_mapping=return_offsets_mapping,\n",
    "            )\n",
    "            for encoding in tokens\n",
    "        ]\n",
    "\n",
    "        # Sanitize the output to have dict[list] from list[dict]\n",
    "        sanitized = {}\n",
    "        for key in tokens[0].keys():\n",
    "            stack = [e for item in tokens for e in item[key]]\n",
    "            if return_tensors == \"tf\":\n",
    "                stack = tf.stack(stack, axis=0)\n",
    "            elif return_tensors == \"pt\":\n",
    "                stack = torch.stack(stack, dim=0)\n",
    "            elif not return_tensors and len(stack) == 1:\n",
    "                stack = stack[0]\n",
    "\n",
    "            sanitized[key] = stack\n",
    "\n",
    "        # If returning overflowing tokens, we need to return a mapping\n",
    "        # from the batch idx to the original sample\n",
    "        if return_overflowing_tokens:\n",
    "            overflow_to_sample_mapping = [\n",
    "                i if len(item[\"input_ids\"]) == 1 else [i] * len(item[\"input_ids\"]) for i, item in enumerate(tokens)\n",
    "            ]\n",
    "            sanitized[\"overflow_to_sample_mapping\"] = overflow_to_sample_mapping\n",
    "        return sanitized\n",
    "\n",
    "    def encode_plus(\n",
    "        self,\n",
    "        text: str,\n",
    "        text_pair: Optional[str] = None,\n",
    "        add_special_tokens: bool = False,\n",
    "        max_length: Optional[int] = None,\n",
    "        pad_to_max_length: bool = False,\n",
    "        stride: int = 0,\n",
    "        truncation_strategy: str = \"longest_first\",\n",
    "        return_tensors: Optional[bool] = None,\n",
    "        return_token_type_ids: Optional[bool] = None,\n",
    "        return_attention_mask: Optional[bool] = None,\n",
    "        return_overflowing_tokens: bool = False,\n",
    "        return_special_tokens_mask: bool = False,\n",
    "        return_offsets_mapping: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        batched_input = [(text, text_pair)] if text_pair else [text]\n",
    "        batched_output = self.batch_encode_plus(\n",
    "            batched_input,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            truncation_strategy=truncation_strategy,\n",
    "            return_tensors=return_tensors,\n",
    "            return_token_type_ids=return_token_type_ids,\n",
    "            return_attention_mask=return_attention_mask,\n",
    "            return_overflowing_tokens=return_overflowing_tokens,\n",
    "            return_special_tokens_mask=return_special_tokens_mask,\n",
    "            return_offsets_mapping=return_offsets_mapping,\n",
    "            pad_to_max_length=pad_to_max_length,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        # Return tensor is None, then we can remove the leading batch axis\n",
    "        if not return_tensors:\n",
    "            return {key: value[0] if isinstance(value[0], list) else value for key, value in batched_output.items()}\n",
    "        else:\n",
    "            return batched_output\n",
    "\n",
    "    def decode(self, token_ids, skip_special_tokens=False, clean_up_tokenization_spaces=True):\n",
    "        text = self.tokenizer.decode(token_ids, skip_special_tokens)\n",
    "\n",
    "        if clean_up_tokenization_spaces:\n",
    "            clean_text = self.clean_up_tokenization(text)\n",
    "            return clean_text\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        if os.path.isdir(save_directory):\n",
    "            files = self._tokenizer.save(save_directory)\n",
    "        else:\n",
    "            folder, file = os.path.split(os.path.abspath(save_directory))\n",
    "            files = self._tokenizer.save(folder, name=file)\n",
    "\n",
    "        return tuple(files)\n",
    "\n",
    "\n",
    "def trim_batch(\n",
    "    input_ids, pad_token_id, attention_mask=None,\n",
    "):\n",
    "    \"\"\"Remove columns that are populated exclusively by pad_token_id\"\"\"\n",
    "    keep_column_mask = input_ids.ne(pad_token_id).any(dim=0)\n",
    "    if attention_mask is None:\n",
    "        return input_ids[:, keep_column_mask]\n",
    "    else:\n",
    "        return (input_ids[:, keep_column_mask], attention_mask[:, keep_column_mask])\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\n",
    "    \"vocab_file\": \"vocab.json\",\n",
    "    \"merges_file\": \"merges.txt\",\n",
    "}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json\",\n",
    "        \"gpt2-medium\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-vocab.json\",\n",
    "        \"gpt2-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-vocab.json\",\n",
    "        \"gpt2-xl\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-vocab.json\",\n",
    "        \"distilgpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-vocab.json\",\n",
    "    },\n",
    "    \"merges_file\": {\n",
    "        \"gpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt\",\n",
    "        \"gpt2-medium\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-medium-merges.txt\",\n",
    "        \"gpt2-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-large-merges.txt\",\n",
    "        \"gpt2-xl\": \"https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-xl-merges.txt\",\n",
    "        \"distilgpt2\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilgpt2-merges.txt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"gpt2\": 1024,\n",
    "    \"gpt2-medium\": 1024,\n",
    "    \"gpt2-large\": 1024,\n",
    "    \"gpt2-xl\": 1024,\n",
    "    \"distilgpt2\": 1024,\n",
    "}\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a mapping to unicode strings.\n",
    "    We specifically avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    \"\"\"\n",
    "    bs = (\n",
    "        list(range(ord(\"!\"), ord(\"~\") + 1)) + list(range(ord(\"\"), ord(\"\") + 1)) + list(range(ord(\"\"), ord(\"\") + 1))\n",
    "    )\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2 ** 8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2 ** 8 + n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "class GPT2Tokenizer(PreTrainedTokenizer):\n",
    "    \"\"\"\n",
    "    GPT-2 BPE tokenizer. Peculiarities:\n",
    "\n",
    "    - Byte-level Byte-Pair-Encoding\n",
    "    - Requires a space to start the input string => the encoding methods should be called with the\n",
    "      ``add_prefix_space`` flag set to ``True``.\n",
    "      Otherwise, this tokenizer ``encode`` and ``decode`` method will not conserve\n",
    "      the absence of a space at the beginning of a string:\n",
    "\n",
    "    ::\n",
    "\n",
    "        tokenizer.decode(tokenizer.encode(\"Hello\")) = \" Hello\"\n",
    "\n",
    "    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the methods. Users\n",
    "    should refer to the superclass for more information regarding methods.\n",
    "\n",
    "    Args:\n",
    "        vocab_file (:obj:`str`):\n",
    "            Path to the vocabulary file.\n",
    "        merges_file (:obj:`str`):\n",
    "            Path to the merges file.\n",
    "        errors (:obj:`str`, `optional`, defaults to \"replace\"):\n",
    "            Paradigm to follow when decoding bytes to UTF-8. See `bytes.decode\n",
    "            <https://docs.python.org/3/library/stdtypes.html#bytes.decode>`__ for more information.\n",
    "        unk_token (:obj:`string`, `optional`, defaults to `<|endoftext|>`):\n",
    "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
    "            token instead.\n",
    "        bos_token (:obj:`string`, `optional`, defaults to `<|endoftext|>`):\n",
    "            The beginning of sequence token.\n",
    "        eos_token (:obj:`string`, `optional`, defaults to `<|endoftext|>`):\n",
    "            The end of sequence token.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        merges_file,\n",
    "        errors=\"replace\",\n",
    "        unk_token=\"<|endoftext|>\",\n",
    "        bos_token=\"<|endoftext|>\",\n",
    "        eos_token=\"<|endoftext|>\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(bos_token=bos_token, eos_token=eos_token, unk_token=unk_token, **kwargs)\n",
    "        self.max_len_single_sentence = (\n",
    "            self.max_len\n",
    "        )  # no default special tokens - you can update this value if you add special tokens\n",
    "        self.max_len_sentences_pair = (\n",
    "            self.max_len\n",
    "        )  # no default special tokens - you can update this value if you add special tokens\n",
    "\n",
    "        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n",
    "            self.encoder = json.load(vocab_handle)\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.errors = errors  # how to handle errors in decoding\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        with open(merges_file, encoding=\"utf-8\") as merges_handle:\n",
    "            bpe_merges = merges_handle.read().split(\"\\n\")[1:-1]\n",
    "        bpe_merges = [tuple(merge.split()) for merge in bpe_merges]\n",
    "        self.bpe_ranks = dict(zip(bpe_merges, range(len(bpe_merges))))\n",
    "        self.cache = {}\n",
    "\n",
    "        # Should haved added re.IGNORECASE so BPE merges can happen for capitalized versions of contractions\n",
    "        self.pat = re.compile(r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\")\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.encoder)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return dict(self.encoder, **self.added_tokens_encoder)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key=lambda pair: self.bpe_ranks.get(pair, float(\"inf\")))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                except ValueError:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "                else:\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "\n",
    "                if word[i] == first and i < len(word) - 1 and word[i + 1] == second:\n",
    "                    new_word.append(first + second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = \" \".join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        \"\"\" Tokenize a string. \"\"\"\n",
    "        bpe_tokens = []\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = \"\".join(\n",
    "                self.byte_encoder[b] for b in token.encode(\"utf-8\")\n",
    "            )  # Maps all our bytes to unicode strings, avoiding controle tokens of the BPE (spaces in our case)\n",
    "            bpe_tokens.extend(bpe_token for bpe_token in self.bpe(token).split(\" \"))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        \"\"\" Converts a token (str) in an id using the vocab. \"\"\"\n",
    "        return self.encoder.get(token, self.encoder.get(self.unk_token))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n",
    "        return self.decoder.get(index)\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        \"\"\" Converts a sequence of tokens (string) in a single string. \"\"\"\n",
    "        text = \"\".join(tokens)\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode(\"utf-8\", errors=self.errors)\n",
    "        return text\n",
    "\n",
    "    def save_vocabulary(self, save_directory):\n",
    "        \"\"\"\n",
    "        Save the vocabulary and special tokens file to a directory.\n",
    "\n",
    "        Args:\n",
    "            save_directory (:obj:`str`):\n",
    "                The directory in which to save the vocabulary.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`Tuple(str)`: Paths to the files saved.\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(save_directory):\n",
    "            logger.error(\"Vocabulary path ({}) should be a directory\".format(save_directory))\n",
    "            return\n",
    "        vocab_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\"vocab_file\"])\n",
    "        merge_file = os.path.join(save_directory, VOCAB_FILES_NAMES[\"merges_file\"])\n",
    "\n",
    "        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(self.encoder, ensure_ascii=False))\n",
    "\n",
    "        index = 0\n",
    "        with open(merge_file, \"w\", encoding=\"utf-8\") as writer:\n",
    "            writer.write(\"#version: 0.2\\n\")\n",
    "            for bpe_tokens, token_index in sorted(self.bpe_ranks.items(), key=lambda kv: kv[1]):\n",
    "                if index != token_index:\n",
    "                    logger.warning(\n",
    "                        \"Saving vocabulary to {}: BPE merge indices are not consecutive.\"\n",
    "                        \" Please check that the tokenizer is not corrupted!\".format(merge_file)\n",
    "                    )\n",
    "                    index = token_index\n",
    "                writer.write(\" \".join(bpe_tokens) + \"\\n\")\n",
    "                index += 1\n",
    "\n",
    "        return vocab_file, merge_file\n",
    "\n",
    "    def prepare_for_tokenization(self, text, **kwargs):\n",
    "        if \"add_prefix_space\" in kwargs and kwargs[\"add_prefix_space\"]:\n",
    "            return \" \" + text\n",
    "        return text\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "VOCAB_FILES_NAMES = {\n",
    "    \"vocab_file\": \"vocab.json\",\n",
    "    \"merges_file\": \"merges.txt\",\n",
    "}\n",
    "\n",
    "PRETRAINED_VOCAB_FILES_MAP = {\n",
    "    \"vocab_file\": {\n",
    "        \"roberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json\",\n",
    "        \"roberta-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json\",\n",
    "        \"roberta-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-vocab.json\",\n",
    "        \"distilroberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-vocab.json\",\n",
    "        \"roberta-base-openai-detector\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json\",\n",
    "        \"roberta-large-openai-detector\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json\",\n",
    "    },\n",
    "    \"merges_file\": {\n",
    "        \"roberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt\",\n",
    "        \"roberta-large\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt\",\n",
    "        \"roberta-large-mnli\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-mnli-merges.txt\",\n",
    "        \"distilroberta-base\": \"https://s3.amazonaws.com/models.huggingface.co/bert/distilroberta-base-merges.txt\",\n",
    "        \"roberta-base-openai-detector\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt\",\n",
    "        \"roberta-large-openai-detector\": \"https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt\",\n",
    "    },\n",
    "}\n",
    "\n",
    "PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = {\n",
    "    \"roberta-base\": 512,\n",
    "    \"roberta-large\": 512,\n",
    "    \"roberta-large-mnli\": 512,\n",
    "    \"distilroberta-base\": 512,\n",
    "    \"roberta-base-openai-detector\": 512,\n",
    "    \"roberta-large-openai-detector\": 512,\n",
    "}\n",
    "\n",
    "\n",
    "class RobertaTokenizer(GPT2Tokenizer):\n",
    "    \"\"\"\n",
    "    Constructs a RoBERTa BPE tokenizer, derived from the GPT-2 tokenizer. Peculiarities:\n",
    "\n",
    "    - Byte-level Byte-Pair-Encoding\n",
    "    - Requires a space to start the input string => the encoding methods should be called with the\n",
    "      ``add_prefix_space`` flag set to ``True``.\n",
    "      Otherwise, this tokenizer ``encode`` and ``decode`` method will not conserve\n",
    "      the absence of a space at the beginning of a string:\n",
    "\n",
    "    ::\n",
    "\n",
    "        tokenizer.decode(tokenizer.encode(\"Hello\")) = \" Hello\"\n",
    "\n",
    "    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the methods. Users\n",
    "    should refer to the superclass for more information regarding methods.\n",
    "\n",
    "    Args:\n",
    "        vocab_file (:obj:`str`):\n",
    "            Path to the vocabulary file.\n",
    "        merges_file (:obj:`str`):\n",
    "            Path to the merges file.\n",
    "        errors (:obj:`str`, `optional`, defaults to \"replace\"):\n",
    "            Paradigm to follow when decoding bytes to UTF-8. See `bytes.decode\n",
    "            <https://docs.python.org/3/library/stdtypes.html#bytes.decode>`__ for more information.\n",
    "        bos_token (:obj:`string`, `optional`, defaults to \"<s>\"):\n",
    "            The beginning of sequence token that was used during pre-training. Can be used a sequence classifier token.\n",
    "\n",
    "            .. note::\n",
    "\n",
    "                When building a sequence using special tokens, this is not the token that is used for the beginning\n",
    "                of sequence. The token used is the :obj:`cls_token`.\n",
    "        eos_token (:obj:`string`, `optional`, defaults to \"</s>\"):\n",
    "            The end of sequence token.\n",
    "\n",
    "            .. note::\n",
    "\n",
    "                When building a sequence using special tokens, this is not the token that is used for the end\n",
    "                of sequence. The token used is the :obj:`sep_token`.\n",
    "        sep_token (:obj:`string`, `optional`, defaults to \"</s>\"):\n",
    "            The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences\n",
    "            for sequence classification or for a text and a question for question answering.\n",
    "            It is also used as the last token of a sequence built with special tokens.\n",
    "        cls_token (:obj:`string`, `optional`, defaults to \"<s>\"):\n",
    "            The classifier token which is used when doing sequence classification (classification of the whole\n",
    "            sequence instead of per-token classification). It is the first token of the sequence when built with\n",
    "            special tokens.\n",
    "        unk_token (:obj:`string`, `optional`, defaults to \"<unk>\"):\n",
    "            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
    "            token instead.\n",
    "        pad_token (:obj:`string`, `optional`, defaults to \"<pad>\"):\n",
    "            The token used for padding, for example when batching sequences of different lengths.\n",
    "        mask_token (:obj:`string`, `optional`, defaults to \"<mask>\"):\n",
    "            The token used for masking values. This is the token used when training this model with masked language\n",
    "            modeling. This is the token which the model will try to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    vocab_files_names = VOCAB_FILES_NAMES\n",
    "    pretrained_vocab_files_map = PRETRAINED_VOCAB_FILES_MAP\n",
    "    max_model_input_sizes = PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES\n",
    "    model_input_names = [\"attention_mask\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_file,\n",
    "        merges_file,\n",
    "        errors=\"replace\",\n",
    "        bos_token=\"<s>\",\n",
    "        eos_token=\"</s>\",\n",
    "        sep_token=\"</s>\",\n",
    "        cls_token=\"<s>\",\n",
    "        unk_token=\"<unk>\",\n",
    "        pad_token=\"<pad>\",\n",
    "        mask_token=\"<mask>\",\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(\n",
    "            vocab_file=vocab_file,\n",
    "            merges_file=merges_file,\n",
    "            errors=errors,\n",
    "            bos_token=bos_token,\n",
    "            eos_token=eos_token,\n",
    "            unk_token=unk_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            **kwargs,\n",
    "        )\n",
    "        self.max_len_single_sentence = self.max_len - 2  # take into account special tokens\n",
    "        self.max_len_sentences_pair = self.max_len - 4  # take into account special tokens\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Build model inputs from a sequence or a pair of sequence for sequence classification tasks\n",
    "        by concatenating and adding special tokens.\n",
    "        A RoBERTa sequence has the following format:\n",
    "\n",
    "        - single sequence: ``<s> X </s>``\n",
    "        - pair of sequences: ``<s> A </s></s> B </s>``\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of IDs to which the special tokens will be added\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.\n",
    "        \"\"\"\n",
    "        if token_ids_1 is None:\n",
    "            return [self.cls_token_id] + token_ids_0 + [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        sep = [self.sep_token_id]\n",
    "        return cls + token_ids_0 + sep + sep + token_ids_1 + sep\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None, already_has_special_tokens: bool = False\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n",
    "        special tokens using the tokenizer ``prepare_for_model`` or ``encode_plus`` methods.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of ids.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "            already_has_special_tokens (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "                Set to True if the token list is already formatted with special tokens for the model\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: A list of integers in the range [0, 1]: 0 for a special token, 1 for a sequence token.\n",
    "        \"\"\"\n",
    "        if already_has_special_tokens:\n",
    "            if token_ids_1 is not None:\n",
    "                raise ValueError(\n",
    "                    \"You should not supply a second sequence if the provided sequence of \"\n",
    "                    \"ids is already formated with special tokens for the model.\"\n",
    "                )\n",
    "            return list(map(lambda x: 1 if x in [self.sep_token_id, self.cls_token_id] else 0, token_ids_0))\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            return [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        return [1] + ([0] * len(token_ids_0)) + [1, 1] + ([0] * len(token_ids_1)) + [1]\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        \"\"\"\n",
    "        Creates a mask from the two sequences passed to be used in a sequence-pair classification task.\n",
    "        RoBERTa does not make use of token type ids, therefore a list of zeros is returned.\n",
    "\n",
    "        Args:\n",
    "            token_ids_0 (:obj:`List[int]`):\n",
    "                List of ids.\n",
    "            token_ids_1 (:obj:`List[int]`, `optional`, defaults to :obj:`None`):\n",
    "                Optional second list of IDs for sequence pairs.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`List[int]`: List of zeros.\n",
    "\n",
    "        \"\"\"\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        if token_ids_1 is None:\n",
    "            return len(cls + token_ids_0 + sep) * [0]\n",
    "        return len(cls + token_ids_0 + sep + sep + token_ids_1 + sep) * [0]\n",
    "\n",
    "    def prepare_for_tokenization(self, text, add_special_tokens=False, **kwargs):\n",
    "        if \"add_prefix_space\" in kwargs:\n",
    "            add_prefix_space = kwargs[\"add_prefix_space\"]\n",
    "        else:\n",
    "            add_prefix_space = add_special_tokens\n",
    "        if add_prefix_space and not text[0].isspace():\n",
    "            text = \" \" + text\n",
    "        return text\n",
    "\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bef66cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T11:40:46.314414Z",
     "iopub.status.busy": "2024-06-02T11:40:46.313869Z",
     "iopub.status.idle": "2024-06-02T11:40:46.363892Z",
     "shell.execute_reply": "2024-06-02T11:40:46.363006Z"
    },
    "papermill": {
     "duration": 0.059791,
     "end_time": "2024-06-02T11:40:46.365950",
     "exception": false,
     "start_time": "2024-06-02T11:40:46.306159",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import enum\n",
    "import sys\n",
    "\n",
    "sys.path += ['./']\n",
    "import os\n",
    "import torch\n",
    "import gzip\n",
    "import pickle\n",
    "import subprocess\n",
    "import csv\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import argparse\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def pad_input_ids(input_ids, max_length,\n",
    "                  pad_on_left=False,\n",
    "                  pad_token=0):\n",
    "    padding_length = max_length - len(input_ids)\n",
    "    padding_id = [pad_token] * padding_length\n",
    "\n",
    "    if padding_length <= 0:\n",
    "        input_ids = input_ids[:max_length]\n",
    "    else:\n",
    "        if pad_on_left:\n",
    "            input_ids = padding_id + input_ids\n",
    "        else:\n",
    "            input_ids = input_ids + padding_id\n",
    "\n",
    "    return input_ids\n",
    "\n",
    "\n",
    "def tokenize_to_file(args, in_path, output_dir, line_fn, max_length, begin_idx, end_idx):\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\n",
    "        args.model_name_or_path, do_lower_case = True, cache_dir=None)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    data_cnt = end_idx - begin_idx\n",
    "    ids_array = np.memmap(\n",
    "        os.path.join(output_dir, \"ids.memmap\"),\n",
    "        shape=(data_cnt, ), mode='w+', dtype=np.int32)\n",
    "    token_ids_array = np.memmap(\n",
    "        os.path.join(output_dir, \"token_ids.memmap\"),\n",
    "        shape=(data_cnt, max_length), mode='w+', dtype=np.int32)\n",
    "    token_length_array = np.memmap(\n",
    "        os.path.join(output_dir, \"lengths.memmap\"),\n",
    "        shape=(data_cnt, ), mode='w+', dtype=np.int32)\n",
    "    pbar = tqdm(total=end_idx-begin_idx, desc=f\"Tokenizing\")\n",
    "    for idx, line in enumerate(open(in_path, 'r')):\n",
    "        if idx < begin_idx:\n",
    "            continue\n",
    "        if idx >= end_idx:\n",
    "            break\n",
    "        qid_or_pid, token_ids, length = line_fn(args, line, tokenizer)\n",
    "        write_idx = idx - begin_idx\n",
    "        ids_array[write_idx] = qid_or_pid\n",
    "        token_ids_array[write_idx, :] = token_ids\n",
    "        token_length_array[write_idx] = length\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    assert write_idx == data_cnt - 1\n",
    "\n",
    "\n",
    "def multi_file_process(args, num_process, in_path, out_path, line_fn, max_length):\n",
    "    output_linecnt = subprocess.check_output([\"wc\", \"-l\", in_path]).decode(\"utf-8\")\n",
    "    print(\"line cnt\", output_linecnt)\n",
    "    all_linecnt = int(output_linecnt.split()[0])\n",
    "    run_arguments = []\n",
    "    for i in range(num_process):\n",
    "        begin_idx = round(all_linecnt * i / num_process)\n",
    "        end_idx = round(all_linecnt * (i+1) / num_process)\n",
    "        output_dir = f\"{out_path}_split_{i}\"\n",
    "        run_arguments.append((\n",
    "                args, in_path, output_dir, line_fn,\n",
    "                max_length, begin_idx, end_idx\n",
    "            ))\n",
    "    pool = multiprocessing.Pool(processes=num_process)\n",
    "    pool.starmap(tokenize_to_file, run_arguments)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    splits_dir = [a[2] for a in run_arguments]\n",
    "    return splits_dir, all_linecnt\n",
    "\n",
    "\n",
    "def write_query_rel(args, pid2offset, qid2offset_file, query_file, positive_id_file, out_query_file, standard_qrel_file):\n",
    "\n",
    "    print( \"Writing query files \" + str(out_query_file) +\n",
    "        \" and \" + str(standard_qrel_file))\n",
    "    query_collection_path = os.path.join(args.data_dir,query_file)\n",
    "    if positive_id_file is None:\n",
    "        query_positive_id = None\n",
    "        valid_query_num = int(subprocess.check_output(\n",
    "            [\"wc\", \"-l\", query_collection_path]).decode(\"utf-8\").split()[0])\n",
    "    else:\n",
    "        query_positive_id = set()\n",
    "        query_positive_id_path = os.path.join(\n",
    "            args.data_dir,\n",
    "            positive_id_file,\n",
    "        )\n",
    "\n",
    "        print(\"Loading query_2_pos_docid\")\n",
    "        for line in open(query_positive_id_path, 'r', encoding='utf8'):\n",
    "            query_positive_id.add(int(line.split()[0]))\n",
    "        valid_query_num = len(query_positive_id)\n",
    "\n",
    "    out_query_path = os.path.join(args.out_data_dir,out_query_file,)\n",
    "\n",
    "    qid2offset = {}\n",
    "\n",
    "    print('start query file split processing')\n",
    "    splits_dir_lst, _ = multi_file_process(\n",
    "        args, args.threads, query_collection_path,\n",
    "        out_query_path, QueryPreprocessingFn,\n",
    "        args.max_query_length\n",
    "        )\n",
    "\n",
    "    print('start merging splits')\n",
    "\n",
    "    token_ids_array = np.memmap(\n",
    "        out_query_path+\".memmap\",\n",
    "        shape=(valid_query_num, args.max_query_length), mode='w+', dtype=np.int32)\n",
    "    token_length_array = []\n",
    "\n",
    "    idx = 0\n",
    "    for split_dir in splits_dir_lst:\n",
    "        ids_array = np.memmap(\n",
    "            os.path.join(split_dir, \"ids.memmap\"), mode='r', dtype=np.int32)\n",
    "        split_token_ids_array = np.memmap(\n",
    "            os.path.join(split_dir, \"token_ids.memmap\"), mode='r', dtype=np.int32)\n",
    "        split_token_ids_array = split_token_ids_array.reshape(len(ids_array), -1)\n",
    "        split_token_length_array = np.memmap(\n",
    "            os.path.join(split_dir, \"lengths.memmap\"), mode='r', dtype=np.int32)\n",
    "        for q_id, token_ids, length in zip(ids_array, split_token_ids_array, split_token_length_array):\n",
    "            if query_positive_id is not None and q_id not in query_positive_id:\n",
    "                # exclude the query as it is not in label set\n",
    "                continue\n",
    "            token_ids_array[idx, :] = token_ids\n",
    "            token_length_array.append(length) \n",
    "            qid2offset[q_id] = idx\n",
    "            idx += 1\n",
    "            if idx < 3:\n",
    "                print(str(idx) + \" \" + str(q_id))\n",
    "    assert len(token_length_array) == len(token_ids_array) == idx\n",
    "    np.save(out_query_path+\"_length.npy\", np.array(token_length_array))\n",
    "\n",
    "    qid2offset_path = os.path.join(\n",
    "        args.out_data_dir,\n",
    "        qid2offset_file,\n",
    "    )\n",
    "    with open(qid2offset_path, 'wb') as handle:\n",
    "        pickle.dump(qid2offset, handle, protocol=4)\n",
    "    print(\"done saving qid2offset\")\n",
    "\n",
    "    print(\"Total lines written: \" + str(idx))\n",
    "    meta = {'type': 'int32', 'total_number': idx,\n",
    "            'embedding_size': args.max_query_length}\n",
    "    with open(out_query_path + \"_meta\", 'w') as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "    if positive_id_file is None:\n",
    "        print(\"No qrels file provided\")\n",
    "        return\n",
    "    print(\"Writing qrels\")\n",
    "    with open(os.path.join(args.out_data_dir, standard_qrel_file), \"w\", encoding='utf-8') as qrel_output: \n",
    "        out_line_count = 0\n",
    "        for line in open(query_positive_id_path, 'r', encoding='utf8'):\n",
    "            topicid, _, docid, rel = line.split()\n",
    "            topicid = int(topicid)\n",
    "            if args.data_type == 0:\n",
    "                docid = int(docid)\n",
    "            else:\n",
    "                docid = int(docid)\n",
    "            qrel_output.write(str(qid2offset[topicid]) +\n",
    "                         \"\\t0\\t\" + str(pid2offset[docid]) +\n",
    "                         \"\\t\" + rel + \"\\n\")\n",
    "            out_line_count += 1\n",
    "        print(\"Total lines written: \" + str(out_line_count))\n",
    "\n",
    "\n",
    "def preprocess(args):\n",
    "    \n",
    "    pid2offset = {}\n",
    "    if args.data_type == 0:\n",
    "        in_passage_path = os.path.join(\n",
    "            args.data_dir,\n",
    "            \"wiki_musique_corpus.tsv\",\n",
    "        )\n",
    "    else:\n",
    "        in_passage_path = os.path.join(\n",
    "            args.data_dir,\n",
    "            \"collection.tsv\",\n",
    "        )\n",
    "\n",
    "    out_passage_path = os.path.join(\n",
    "        args.out_data_dir,\n",
    "        \"passages\",\n",
    "    )\n",
    "\n",
    "    if os.path.exists(out_passage_path):\n",
    "        print(\"preprocessed data already exist, exit preprocessing\")\n",
    "        return\n",
    "\n",
    "    print('start passage file split processing')\n",
    "    splits_dir_lst, all_linecnt = multi_file_process(\n",
    "        args, args.threads, in_passage_path,\n",
    "        out_passage_path, PassagePreprocessingFn,\n",
    "        args.max_seq_length\n",
    "        )\n",
    "\n",
    "    token_ids_array = np.memmap(\n",
    "        out_passage_path+\".memmap\",\n",
    "        shape=(all_linecnt, args.max_seq_length), mode='w+', dtype=np.int32)\n",
    "    token_length_array = []\n",
    "\n",
    "    idx = 0\n",
    "    out_line_count = 0\n",
    "    print('start merging splits')\n",
    "    for split_dir in splits_dir_lst:\n",
    "        ids_array = np.memmap(\n",
    "            os.path.join(split_dir, \"ids.memmap\"), mode='r', dtype=np.int32)\n",
    "        split_token_ids_array = np.memmap(\n",
    "            os.path.join(split_dir, \"token_ids.memmap\"), mode='r', dtype=np.int32)\n",
    "        split_token_ids_array = split_token_ids_array.reshape(len(ids_array), -1)\n",
    "        split_token_length_array = np.memmap(\n",
    "            os.path.join(split_dir, \"lengths.memmap\"), mode='r', dtype=np.int32)\n",
    "        for p_id, token_ids, length in zip(ids_array, split_token_ids_array, split_token_length_array):\n",
    "            token_ids_array[idx, :] = token_ids\n",
    "            token_length_array.append(length) \n",
    "            pid2offset[p_id] = idx\n",
    "            idx += 1\n",
    "            if idx < 3:\n",
    "                print(str(idx) + \" \" + str(p_id))\n",
    "            out_line_count += 1\n",
    "    assert len(token_length_array) == len(token_ids_array) == idx\n",
    "    np.save(out_passage_path+\"_length.npy\", np.array(token_length_array))\n",
    "\n",
    "    print(\"Total lines written: \" + str(out_line_count))\n",
    "    meta = {\n",
    "        'type': 'int32',\n",
    "        'total_number': out_line_count,\n",
    "        'embedding_size': args.max_seq_length}\n",
    "    with open(out_passage_path + \"_meta\", 'w') as f:\n",
    "        json.dump(meta, f)\n",
    "    \n",
    "    pid2offset_path = os.path.join(\n",
    "        args.out_data_dir,\n",
    "        \"pid2offset.pickle\",\n",
    "    )\n",
    "    with open(pid2offset_path, 'wb') as handle:\n",
    "        pickle.dump(pid2offset, handle, protocol=4)\n",
    "    \n",
    "    print(\"done saving pid2offset\")\n",
    "    \n",
    "    if args.data_type == 0: \n",
    "        \n",
    "        write_query_rel(\n",
    "            args,\n",
    "            pid2offset,\n",
    "            \"test-qid2offset.pickle\",\n",
    "            \"/kaggle/input/wiki-tsv/train-queries.tsv\",\n",
    "            \"/kaggle/input/wiki-tsv/train-qrels.tsv\",\n",
    "            \"train-queries\",\n",
    "            \"train-qrels.tsv\")\n",
    "        \n",
    "        write_query_rel(\n",
    "             args,\n",
    "             pid2offset,\n",
    "             \"train-qid2offset.pickle\",\n",
    "             \"/kaggle/input/wiki-tsv/dev-queries.tsv\",\n",
    "              None,\n",
    "             \"queries\",\n",
    "             \"qrels.tsv\")\n",
    "\n",
    "\n",
    "def PassagePreprocessingFn(args, line, tokenizer):\n",
    "    if args.data_type == 0:\n",
    "        line_arr = line.split('\\t')\n",
    "        p_id = int(line_arr[0])  # remove \"D\"\n",
    "\n",
    "        url = \"0\"\n",
    "        title = line_arr[1].rstrip()\n",
    "        p_text = line_arr[2].rstrip()\n",
    "        # NOTE: This linke is copied from ANCE, \n",
    "        # but I think it's better to use <s> as the separator, \n",
    "        full_text = url + \"<sep>\" + title + \"<sep>\" + p_text\n",
    "        # keep only first 10000 characters, should be sufficient for any\n",
    "        # experiment that uses less than 500 - 1k tokens\n",
    "        full_text = full_text[:args.max_doc_character]\n",
    "    else:\n",
    "        line = line.strip()\n",
    "        line_arr = line.split('\\t')\n",
    "        p_id = int(line_arr[0])\n",
    "\n",
    "        p_text = line_arr[1].rstrip()\n",
    "        # keep only first 10000 characters, should be sufficient for any\n",
    "        # experiment that uses less than 500 - 1k tokens\n",
    "        full_text = p_text[:args.max_doc_character]\n",
    "    passage = tokenizer.encode(\n",
    "        full_text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=args.max_seq_length,\n",
    "        truncation=True\n",
    "    )\n",
    "    passage_len = min(len(passage), args.max_seq_length)\n",
    "    input_id_b = pad_input_ids(passage, args.max_seq_length)\n",
    "\n",
    "    return p_id, input_id_b, passage_len\n",
    "\n",
    "\n",
    "def QueryPreprocessingFn(args, line, tokenizer):\n",
    "    line_arr = line.split('\\t')\n",
    "    q_id = int(line_arr[0])\n",
    "\n",
    "    passage = tokenizer.encode(\n",
    "        line_arr[1].rstrip(),\n",
    "        add_special_tokens=True,\n",
    "        max_length=args.max_query_length,\n",
    "        truncation=True)\n",
    "    passage_len = min(len(passage), args.max_query_length)\n",
    "    input_id_b = pad_input_ids(passage, args.max_query_length)\n",
    "\n",
    "    return q_id, input_id_b, passage_len\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4281ef39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-02T11:40:46.379838Z",
     "iopub.status.busy": "2024-06-02T11:40:46.379512Z",
     "iopub.status.idle": "2024-06-02T11:47:21.632595Z",
     "shell.execute_reply": "2024-06-02T11:47:21.631485Z"
    },
    "papermill": {
     "duration": 395.26228,
     "end_time": "2024-06-02T11:47:21.634671",
     "exception": false,
     "start_time": "2024-06-02T11:40:46.372391",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start passage file split processing\n",
      "line cnt 563424 /kaggle/input/wiki-tsv/wiki_musique_corpus.tsv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Downloading: 100%|| 899k/899k [00:00<00:00, 7.45MB/s]\n",
      "Downloading: 100%|| 899k/899k [00:00<00:00, 9.08MB/s]\n",
      "Downloading: 100%|| 456k/456k [00:00<00:00, 5.84MB/s]\n",
      "Downloading: 100%|| 899k/899k [00:00<00:00, 8.72MB/s]\n",
      "Downloading: 100%|| 899k/899k [00:00<00:00, 9.26MB/s]\n",
      "Tokenizing:   0%|          | 21/70428 [00:00<12:32, 93.53it/s] \n",
      "Tokenizing:   0%|          | 132/70428 [00:00<06:56, 168.87it/s]\n",
      "Downloading: 100%|| 899k/899k [00:00<00:00, 7.43MB/s]\n",
      "Tokenizing:   0%|          | 190/70428 [00:01<06:40, 175.32it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Downloading: 100%|| 899k/899k [00:00<00:00, 6.94MB/s]\n",
      "Tokenizing:  98%|| 69147/70428 [05:42<00:06, 185.40it/s]\n",
      "Tokenizing: 100%|| 70428/70428 [05:46<00:00, 203.10it/s]\n",
      "Tokenizing: 100%|| 70208/70428 [05:46<00:00, 281.21it/s]\n",
      "Tokenizing: 100%|| 70428/70428 [05:47<00:00, 202.85it/s]\n",
      "Tokenizing: 100%|| 70422/70428 [05:47<00:00, 472.69it/s]\n",
      "\n",
      "Tokenizing: 100%|| 70428/70428 [05:46<00:00, 203.23it/s]\n",
      "Tokenizing: 100%|| 70428/70428 [05:47<00:00, 202.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start merging splits\n",
      "1 0\n",
      "2 1\n",
      "Total lines written: 563424\n",
      "done saving pid2offset\n",
      "Writing query files train-queries and train-qrels.tsv\n",
      "Loading query_2_pos_docid\n",
      "start query file split processing\n",
      "line cnt 167454 /kaggle/input/wiki-tsv/train-queries.tsv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Tokenizing:   0%|          | 40/20932 [00:00<00:52, 399.69it/s]/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Tokenizing:  95%|| 19947/20932 [00:25<00:00, 1031.89it/s]\n",
      "Tokenizing:  99%|| 20637/20932 [00:26<00:00, 795.53it/s]\n",
      "Tokenizing: 100%|| 20932/20932 [00:26<00:00, 795.89it/s] \n",
      "Tokenizing: 100%|| 20932/20932 [00:26<00:00, 789.79it/s]\n",
      "Tokenizing: 100%|| 20932/20932 [00:26<00:00, 785.75it/s] \n",
      "Tokenizing: 100%|| 20931/20931 [00:26<00:00, 785.66it/s] \n",
      "Tokenizing: 100%|| 20932/20932 [00:26<00:00, 786.33it/s]\n",
      "Tokenizing: 100%|| 20931/20931 [00:26<00:00, 776.88it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start merging splits\n",
      "1 0\n",
      "2 1\n",
      "done saving qid2offset\n",
      "Total lines written: 167454\n",
      "Writing qrels\n",
      "Total lines written: 404805\n",
      "Writing query files queries and qrels.tsv\n",
      "start query file split processing\n",
      "line cnt 1200 /kaggle/input/wiki-tsv/dev-queries.tsv\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Tokenizing:  33%|      | 50/150 [00:00<00:00, 498.11it/s]\n",
      "Tokenizing:  81%|  | 121/150 [00:00<00:00, 571.02it/s]\n",
      "\n",
      "Tokenizing: 100%|| 150/150 [00:00<00:00, 778.75it/s]\n",
      "\n",
      "Tokenizing: 100%|| 150/150 [00:00<00:00, 861.84it/s]\n",
      "Tokenizing: 100%|| 150/150 [00:00<00:00, 692.27it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start merging splits\n",
      "1 0\n",
      "2 1\n",
      "done saving qid2offset\n",
      "Total lines written: 1200\n",
      "No qrels file provided\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "args = Namespace(\n",
    "    model_name_or_path = \"roberta-base\",\n",
    "    max_seq_length = 512,\n",
    "    max_query_length = 64,\n",
    "    max_doc_character = 10000,\n",
    "    data_type = 0,\n",
    "    threads = 8\n",
    ")\n",
    "\n",
    "if args.data_type == 0:\n",
    "    args.data_dir = \"/kaggle/input/wiki-tsv\"\n",
    "    args.out_data_dir = \"/kaggle/working/data/doc/preprocess\"\n",
    "else:\n",
    "    args.data_dir = \"/kaggle/input/data/passage/dataset\"\n",
    "    args.out_data_dir = \"/kaggle/working/data/passage/preprocess\"\n",
    "\n",
    "if not os.path.exists(args.out_data_dir):\n",
    "    os.makedirs(args.out_data_dir)\n",
    "\n",
    "preprocess(args)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5125314,
     "sourceId": 8578462,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30716,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 419.767918,
   "end_time": "2024-06-02T11:47:26.480935",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-02T11:40:26.713017",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
